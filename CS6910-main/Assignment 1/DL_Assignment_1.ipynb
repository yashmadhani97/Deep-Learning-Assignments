{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a4e443450634788afecf4567d3a9dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ddc28b993cf4273805d75a09a731ced",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e0fb9e6d2d447d68e515f5276f7addc",
              "IPY_MODEL_8b90e31039a0489da8e8faac6adb66db"
            ]
          }
        },
        "9ddc28b993cf4273805d75a09a731ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e0fb9e6d2d447d68e515f5276f7addc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_31c1021c14dc4038a4603a29435959e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.05MB of 0.05MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43f519b209604feaaa7a390dade3b4d1"
          }
        },
        "8b90e31039a0489da8e8faac6adb66db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92db5e57b63046bdb9c5dbbf9ae223db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3bb01fff9bb3401e9f1bc147b3008b8d"
          }
        },
        "31c1021c14dc4038a4603a29435959e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43f519b209604feaaa7a390dade3b4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92db5e57b63046bdb9c5dbbf9ae223db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3bb01fff9bb3401e9f1bc147b3008b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e48916a1bed445deb235d13b87e3bd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_29beec98759a4907ba1477a837aa6118",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_513b52e6460b47998b8f0cd11fdeef0c",
              "IPY_MODEL_e437c2afb93c40278c0df95503696bbc"
            ]
          }
        },
        "29beec98759a4907ba1477a837aa6118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "513b52e6460b47998b8f0cd11fdeef0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_6e493623ffd84d8f9aefb6974f9fc86a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8554a25024bb465bb578590765e09ae6"
          }
        },
        "e437c2afb93c40278c0df95503696bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76ee111af8ff493b8e4e236f9d8853f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80ed5e0bb31b44a6aaa0c3b7202954e5"
          }
        },
        "6e493623ffd84d8f9aefb6974f9fc86a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8554a25024bb465bb578590765e09ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76ee111af8ff493b8e4e236f9d8853f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80ed5e0bb31b44a6aaa0c3b7202954e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9dd1ef479134753998681680fc77f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c2cb07c5996401591d9ad5bebd930c9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc9760c0705e43b4921708303f758fcc",
              "IPY_MODEL_42eea7a54fc6405eace002a8d2e25478"
            ]
          }
        },
        "9c2cb07c5996401591d9ad5bebd930c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc9760c0705e43b4921708303f758fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_70592613a4724b1e8faa47758c2f487d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e65197daece04acdae3fd4369fa409e5"
          }
        },
        "42eea7a54fc6405eace002a8d2e25478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6717885b6104cbe967e70da9ebeb371",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab61529c64d9444585bd0bce09096aba"
          }
        },
        "70592613a4724b1e8faa47758c2f487d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e65197daece04acdae3fd4369fa409e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6717885b6104cbe967e70da9ebeb371": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab61529c64d9444585bd0bce09096aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXMjn5yXWX5"
      },
      "source": [
        "# Common cell for all questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YlN66_5R1sy"
      },
      "source": [
        "# importing all packages\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from PIL import Image \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\r\n",
        "import math\r\n",
        "from tensorflow.keras import initializers\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import seaborn as sns\r\n",
        "import random"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO4oUUwtOU_O",
        "outputId": "ef1f96fb-8ee7-4c0c-ed39-62d0ec5465d9"
      },
      "source": [
        "# wandb config\r\n",
        "!pip install wandb\r\n",
        "import wandb\r\n",
        "!wandb login"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.22)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20s002\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKTfpjnEST57"
      },
      "source": [
        "# Loading dataset\r\n",
        "df = fashion_mnist.load_data()\r\n",
        "((train_X, train_Y), (test_X, test_Y)) = df"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFvClN1Q4eKq"
      },
      "source": [
        "# Question 1\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKQJlSYpM34"
      },
      "source": [
        "# Extracting unique images classwise \r\n",
        "unique_class_values, unique_class_values_indices = np.unique(train_Y, return_index=True)\r\n",
        "unique_sample_images = train_X[unique_class_values_indices]\r\n",
        "image_labels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEajxDJN2JB"
      },
      "source": [
        "# Wandb config for question 1\r\n",
        "\r\n",
        "# Intialize a new run\r\n",
        "wandb.init(project=\"DL_assignment_1\", name=\"Question_1_images\")\r\n",
        "\r\n",
        "# Log the image\r\n",
        "wandb.log({\"Examples\": [ wandb.Image( plt.imshow( im, cmap=\"Greys\"), caption = label  ) for im,label in zip(unique_sample_images,image_labels )  ]})\r\n",
        "\r\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "Rw6T_YF5r_Cl",
        "outputId": "c5f072cc-50a3-464e-e42b-f8f38d155d8d"
      },
      "source": [
        "# plotting unique images\r\n",
        "fig = plt.figure(figsize=(12., 12.))\r\n",
        "grid = ImageGrid(fig, 111,  # similar to subplot(111)\r\n",
        "                 nrows_ncols=(2, 5),  # creates 2x2 grid of axes\r\n",
        "                 axes_pad=0.1,  # pad between axes in inch.\r\n",
        "                 )\r\n",
        "\r\n",
        "for ax, im in zip(grid, unique_sample_images ):\r\n",
        "    # Iterating over the grid returns the Axes.\r\n",
        "    ax.imshow(im,cmap = 'Greys')\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAEoCAYAAABRkpmIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZhdVZX23w0kTAkhEyHzTAYDJFAEMBGZW5BBWhSQRnxAUVukEW1F0A+6lRa1pVsFW0ODYssgKCgizSCDjAEqJCYhEBIyQOZ5AEwkuL8/UrFrr/Xeuju3bg23zvt7Hh6yVtY9Z9c56+yzc2u9e4UYI4QQQgghhCgCu7T1AIQQQgghhGgttPgVQgghhBCFQYtfIYQQQghRGLT4FUIIIYQQhUGLXyGEEEIIURi0+BVCCCGEEIWhWYvfEMIHQghzQwjzQwiXV2tQQgghhBBCtASh0n1+Qwi7AngVwAkAlgB4AcA5McY5pT7Tq1evOGTIkIrO15Zs27bN+V5//fXE7tmzp4vZc889EzuEkHW+rVu3Jva6detczO677+58++23X0Xnay0WLVqENWvW0EHVQm785S9/cb41a9Y4X6dOnRJ71113dTH23rzzzjtZY9hjjz0S+80333QxLF/t8YcPH551vtaiqdwAaiM//vrXvzrf22+/7XyrVq1KbJYfOXPHu+++63ybN29O7C5duriY/fff3/na21xhqbW5w94blgdr1651vt122y2xe/Xq5WLs3M/mjvXr1zvfhg0bEpvlXe/evZ2vW7duzteeqLXcyIGty+x93rJli4ux7wf2XLP3g807+w6rZaZNm7YmxugSezcWnMlEAPNjjAsAIIRwB4DTAZRc/A4ZMgT19fXNOGVpWLJUa0K3LysAuPjiixP74x//uIs58MADE9smGMAnoNdeey2xb7vtNhczcuRI5/vMZz6T2PZBaGvq6upK/l1L5ka1WLx4sfP97Gc/cz77Atl3331djJ1cVqxY4WJY/o4YMSKxp06d6mJWrlzpfDaHf/3rX7uYtqSp3ACqlx8tOU+89dZbzjd9+nTn+8EPfpDYLD/GjRuX2OxZZv8ofvzxxxP7fe97n4v58pe/7HyVvOxa8lpaam3usAvNadOmuRg2r9vF7gUXXOBi7GKOzR133XWX8/3+979P7H322cfFfPrTn3a+k08+2fnaE+05Nyp9RtgXLcuWLUvsefPmuZhRo0YlNnuu2T+6evTokdj9+vUrO0bA/3zt8R/RIQT/4kbzyh76A3ijkb2kwSeEEEIIIUS7pMUFbyGEi0II9SGE+tWrV7f06UQNodwQTaH8EKVQbohSKDdEDs0pe1gKYGAje0CDLyHGOAXAFACoq6urqMA456v1nK/blyxZ4ny//OUvE/vmm292MexXB/bXWn/84x9dDCuXqARbPgEAzzzzjPNddtllic1+dXHWWWcl9pe+9CUX07dv350dYkVUIzdakyeeeML5Zs6c6Xy77LJL2ZiNGzcmNitVYPV+9tdTffr0cTHs/s2aNcv52jutNXcwbI3mTTfd5GLsr5FZXSf71fKf//znxH7ooYdczI033lh2jGxeGjp0aGLbXASA4447zvlsHfCxxx7rYj7xiU8kdluWVbXl3PGnP/0psb/1rW+5GFuzzX6Nza7fwoULE5vdB/vrb1YCx3LDlkyxcpuf//znzvf//t//S+zTTjutbExb0p7eKznzDVug2zkC2F7b3JjnnnvOxdj5htVrb9q0yfmszoiVa/Tv73+x3x7LHHJpzje/LwAYGUIYGkLoDOBsAPdWZ1hCCCGEEEJUn4q/+Y0xbgshXAzgQQC7Arg5xvhS1UYmhBBCCCFElWlO2QNijPcDuL9KYxFCCCGEEKJFadbit7XIqSuxNSsA8NnPfjaxn376aRdj92Ps3r27i+natavz2bpKtmWZrfNitT2s7srW6bG6PYat5WN7wNr6wt/+9rcu5oMf/KDzff/7388aQ0fG7qEK8P1y7d6/o0ePdjFsj1YLqyG1tVhsX06257R9PmzNOsBzsQiwLcOOP/74xLa1tIC/zmz/XDYv2FrPSZMmuRibaznHAfx9ZroDNsfZucrWMwNe13DppZe6mMMPP9z5ahmWGz/96U8Te8KECS7GbnvH9oBm87rdDpO9e3KOw3z2+WZ1wWw7Trtdnt3jHgC+/e1vJ/ZXvvIVPljhcortFc+0HPa6szWPnZPs3v8Azw1bK8zWDuy+Dxo0yPlqBbU3FkIIIYQQhUGLXyGEEEIIURi0+BVCCCGEEIVBi18hhBBCCFEYakLwlsN5553nfC+9lO68NnjwYBdjhSRsM/LOnTs7nxUebdu2rWwM2yQ6R/iUE8NgYokuXbokNhMT3nfffc535ZVXJjYrpO/ovPzyy87HBEW2gQUTl9mNxplwgOWU3fyciReYuMbm9fTp013MMccc43y1To5Y1uY2AAwbNiyxbXMRAHjnnXfKnosJiOy8wISUu+++e2LniNsAL7TKFTXZ49t5AvA59L3vfc/F/OIXv3A+Nn/WCv/5n//pfEyMZLHPIGtawO6D9dnGFIAXrrFjs3mB5YuFCSttnjOR74wZM5q0AWD8+PFlz18E7DPKBKgsN4YMGZLYN9xwg4uxTbrOPfdcF3PKKac4n12bsOY8TLBvc4PNN+0VffMrhBBCCCEKgxa/QgghhBCiMGjxK4QQQgghCkNN1vy+8cYbzmfrewFg4MCBic3qeW1dJau/W7hwofPZuh1Wl2tr3Wx9DJBXE8jGzWprunXrltisXiyn/o6NyW7sXsRNzFnN0/r1653P1vyyz9kmKSw3WE7ZWmFW28eaY9g8ZxurFwG2efuyZcuczz5L7Bm0zwm77naeAHw9KLuHtvaS1WJu2bLF+WzusWeZzQE2jtX82rpgdi2nTp3qfEcddZTz1QoXXnih8333u99NbKZ/sM+3fW6BvPpIW/sNACtXriz7OdawZq+99ir7uZwxsDnP6mmKWt9r393sWtka7Tlz5rgYpg+yTXRmz57tYuyzzeq82fppzz33TOzFixe7GKZ5sXXIp59+uoth9cPtAX3zK4QQQgghCoMWv0IIIYQQojBo8SuEEEIIIQqDFr9CCCGEEKIwNEvwFkJYBGAzgHcBbIsx1lVjUOV44oknnI9t9G0FKDkbeDOhx//8z/84nxU0MNGDFTr16tXLxbCGBFZ8ktPsAABefPHFxL7qqqtczIABAxKbCa3YdbrxxhsTu4iCt7Vr1zqfFVUCXvzxwgsvuJgVK1Yk9t577501hpyGJ1asBXghBtuE/iMf+UjWGGoZJkBbsGCB8x1wwAGJzZ43Ky5jQjImgrMCN3tvAH+fmSiOPbv2WOxZZnNOjvjKzh3smvzud79zvloWvLHGSEcffXRi33nnnS7mve99b2Kze8Vy0b4jmODNNtlgQjYmRrRjYM0V7LzEYONmjWKKiBW4sSZI9p4OHTrUxcyaNcv5Dj300MRmorh58+YlNmtY9b73vc/5nnrqqcRmjUxOOukk57PzCxu3fYbsPNJWVGO3h2NijMWUjgshhBBCiJpCZQ9CCCGEEKIwNHfxGwE8FEKYFkK4iAWEEC4KIdSHEOrZfqeiuCg3RFMoP0QplBuiFMoNkUNzF7+TY4yHADgJwOdCCK64K8Y4JcZYF2Os6927dzNPJzoSyg3RFMoPUQrlhiiFckPk0Kya3xjj0ob/rwoh3ANgIgCvRqsyt9xyi/MxwYYt8GfdjmzxPntYWKG37a7COsydcMIJic2Kz8eMGeN8tsscEzkxscLZZ5+d2N/4xjdcjL0mTBjBOrJYMR3rEMYEfbWMFRoywduECROczwqfJk6c6GI2bNiQ2K+++mrZGMBfY3avrCAG8CKDpUuXupgiYAUhABeUWhEcewatj4llhw0b5nwjR45MbNaJ0d5X24EJ4CJJOw+yDk/19fXOd/vttyd2165dXYztLMa6V7GfpaNx5plnJvbXv/51F2NFPuyZZPlifaxTm4WJ6awgm8WxroVsPrHz0Ic+9CEXw4R5RcS+T9n9s2uOEIKLYSJROwcw4f/hhx+e2Gzemjx5svNZ4Rr7HOt+a7s+srnUrhXY5gA5nWerTcXf/IYQ9g4hdN3xZwAnAvD99oQQQgghhGgnNOeb3z4A7mn4V8tuAG6LMT5QlVEJIYQQQgjRAlS8+I0xLgBwcBXHIoQQQgghRItSjX1+Wx27ITPAN2W29Sdsw3kLq2VlHHTQQYnN6re+9rWvJTZrDHHhhRc6n61pZnU0rCboj3/8Y2KzOhpbv8XqoNnG+LaWb+bMmS7m2GOPdb5aZsuWLYnN6qxZ/ZLd2JzVXW7atCmxWRMD1kTA1g+zz7H7Z+tDWWOFIsDq3Z5//nnnu+eeexJ7+vTpLuaKK65I7ErFNaxm0z7zLIbllY2zNXkAnzsuu+yyxGbP8uuvv57YbM575ZVXnK+WYQ1B7DPHmtj8y7/8S9ljs+tna7ZZQwn7LLP3A/PZmlH2szFs/echhxyS9bmOTs5zy+ZiWx/Naq9tAx12vh49ergYeyyrOwJ4HbLVgOQ2x7G+nCZM7GdrC2Gi9vkVQgghhBCFQYtfIYQQQghRGLT4FUIIIYQQhUGLXyGEEEIIURhqQvC2fPnyxN5///1dDGtyYYuvmQjAbtzcv3//isbEzr9u3brEvuSSS7KO/aMf/SixmTiJbdZvYRvO28+xwvYcwdSDDz7oYjqa4M2KTVijASY4s6IDtgG8FS9MmzbNxdiN8gHffIHdYzZO2xyD5WsR+O53v+t87B6eeOKJiX3ooYe6GCugHT9+vIthIhF7L9h8ZsWV7H6xzfHtXMEapTz33HPOd/DB6cY9d955p4uxeczyjAloaxmWGxYmXBs1alRis/maXb9u3bolNpuL99prr8RmIiN7HABYtmxZYrOcYvnKxOSCC5Lt88dE5/b9wBrK5AhcrSAb8I20WB5YsTXgx82eY5ZnNj/ZhgE2z1hzHAnehBBCCCGEaEG0+BVCCCGEEIVBi18hhBBCCFEYaqJA69prr01s1qyCbdxsa01sfS/g67VYjY7d3B3wGzWz2jpbY7x27VoXw2prbG0dq/9hdTtPP/10Yq9cudLF2J939erVLobVGNv6JtZopKORU5vFsPeL5V3fvn0Tm9Vv2tpQwNdev/baay6G1U/ZZ4FtrF4ETjnlFOd74AHflf22225L7LvuusvFfOELX0jsO+64w8Ww53Tu3LllY2w+ML0Cu4d27mA1qxdddJHz2bz6zne+U/bYbJN926AHAJ599tnEZrWuHQ1bO8vmAHZvbB0ne6/ZGFsDDHCdgSW3PjtXB1M0cupy2VrFvoNZsxr2PrBzOKvZtrX6J510kothTUqsvoXV97J6cDsHWS0UAIwbNy6xcxuJtTT65lcIIYQQQhQGLX6FEEIIIURh0OJXCCGEEEIUBi1+hRBCCCFEYShb8R5CuBnAKQBWxRjHNfh6APglgCEAFgH4aIzR71xcJWzzhBUrVriY6dOnO5/dTPnNN990MQcddFBiMxHAsGHDnM+KFXKaRbAiclY0bwVn7Nis+NxuaM0K260wjwlp2DhHjhyZ2GeffbaL6WhY0UGuUMfeGyZaqa+vL3sc1vzAiiUOOOAAF7N48WLns8KLXPFeR+Nzn/uc8zFx0MCBAxP78MMPdzG//OUvE/viiy/OGoMVquyxxx4uxs5DOQIYwD/PTEzD5sGePXsmtm3yAXiRJhPTWHEL0PEEbnZ+Zvdm6NChic2a2LA53OYCE8XZZ5m9H1gDBitqZEI5JpLu06eP81nsz5LTHKTWsSIxwOc6uw82hq052LrAwt7dVtBrn9lSx7ZCfyamzRkna/hiP8fmJCayZ89VNcnJ0J8B+IDxXQ7gkRjjSACPNNhCCCGEEEK0a8oufmOMTwBYZ9ynA9ixp80tAD5U5XEJIYQQQghRdSr93USfGOOODd1WACj5e5EQwkUhhPoQQj3bU1YUF+WGaArlhyiFckOUQrkhcmh2YU7cXqzhCzb+7++nxBjrYox1bPN9UVyUG6IplB+iFMoNUQrlhsih0g5vK0MIfWOMy0MIfQGsquagLKeffnqTNuC73gC+eP+HP/yhi/nd736X2KxAnIld9ttvv8RmRdysIL0SWDE4E6VZQQP7V++RRx6Z2D/4wQ+aObqOi73uuSIOG8dy04oxGaNGjXK+qVOnJvaYMWNcjM0DAFi3Lq1cYiKZInDuuec634MPPuh8jz76aGKfc845Luass84qG2OFooCfF5i4xIppcucSK4JjAhQm8LNC2Pnz57uYK6+8MrEXLVrkYu6++27ne+ONNxJ7wIABLqajYQWTTNzG3hm2+xUTtNp7zERqOV0emdCSjbOoc0VjmEiMdW+z4i4mPrbXnc3XTOyV0/XRPu9s7cDusfWx9UVOR0K2GYHNYXbdmDCQCTKrSaXf/N4L4PyGP58P4LfVGY4QQgghhBAtR9nFbwjhdgDPAhgVQlgSQrgQwLUATgghzANwfIMthBBCCCFEu6Zs2UOM0f8ubzvHVXksQgghhBBCtCiV1vy2O1j90uDBgxP7sssuczH33HNPYrNaG1aPsnHjxsRm9Tc5tVKs/ianyQXbqN5eA7YJt20YIkpjc4E1FWA1lXYTc1Z7vc8++5Q9/9ixY53P1qeyvGd167buktV0FYFZs2Y5H2vCYBvbHHPMMS7m4YcfTuwZM2a4GDaf5NTv2meeHYfV81kfq1Vk84mtw/3kJz/pYiZPnpzYw4cPdzGXXnqp8+U0Seho2LrqSvUC7Dm1NZPs2N27d3e+VatSaY59h5WiWtqVjga77natwLQd7D2Sg80FNifYGlz2fmDYcec2nXj11VcTm9U42/xhtbybNm1yvvZa8yuEEEIIIUTNocWvEEIIIYQoDFr8CiGEEEKIwqDFrxBCCCGEKAw1KXjL3bg5R3DWo0ePxM4VrrHi73JjyhU9VEqOMMH+vAx2Le3Pm/PzdzTYz8wERVZAxZoY5IiADjrooLIxTEzH7t/++++f2EW8f4AXaAD8Hto41jTAbk7fuXNnF8OEjfb+2I3xAS9uYXNHjgiObSjPBDd2c3omAly7dm1iWxEl4Js0AF6cy8RYtUTOs2PfGex5Z81GcjqS9erVK7GZqIk11rFzgBXAAXlC3CLC5lR2/+w7mD1HXbt2TezNmze7GJZjdk5g6yB7vpwxAn4OyhVEW6Eay3M7T7LmLuwd2dLom18hhBBCCFEYtPgVQgghhBCFQYtfIYQQQghRGGqy5pfVw+TU09paKcDXWLH6v5zNlnM3oc/B1hexmmM2JlZLY2FNGSxs3C1dr9wesdeBXV/WAMXWfrL7161bt7LnHzNmjPPZWixWv8XulR17S28g3l5htXu2Jg3w94zF2CYy7Lqz2rmc2r0cvUDOc8rOzz5n82jfffd1MZaVK1c6H6vds7XCtV7za68fm/ttrfW6detcDMspVsNvsXWVrJnRhg0bnI/VpFvY87F06dLEHj16tIvp6O8Hti7I0cawumr7rOWuHeycxM5vYY1MWI241QGwmnE2l9jPLVu2zMXYRh/s+VfNrxBCCCGEEC2IFr9CCCGEEKIwaPErhBBCCCEKgxa/QgghhBCiMJQVvIUQbgZwCoBVMcZxDb6rAXwKwI7q/CtijPe31CBzYAXitpCcbe5uN4Vmm8IzoYAt0M45PytQZ5/L2eCeCZZscTs7X0cXJlQTe/1yhEKAF7ew+zBu3Liy52cblFtyNkPP/VwRyBWA2eeECUWt2JEJG3NEMYycpjJs3HaOY2Ni4h17XZgoxV4D1pyDPQ9MkFXL5Nw/K2gdP368ixk+fLjz2YYgbJ5fsmRJYrN5YuTIkc5nj8VEeAMGDHC+119/3fmKhr0vAM91+xzlCMzZO5kdO2c9Y2HzD/ucnRPYmJgozTZOYc+6nadYvi5fvtz5RowY4XzVJGcl9DMAHyD+/4gxjm/4r00XvkIIIYQQQuRQdvEbY3wCgP8nohBCCCGEEDVGc34HfnEIYWYI4eYQQsmNG0MIF4UQ6kMI9Tl7GIrioNwQTaH8EKVQbohSKDdEDpUufv8LwHAA4wEsB/C9UoExxikxxroYY51tKCGKjXJDNIXyQ5RCuSFKodwQOVTU4S3G+LfWPiGEGwHcV7URVUglIhLAC0JYTKWiIltYzoQmDFsgzs6V87NUKngrqhjKYq876+bGJlcrSGGfGzZsWNnzM2GAFV8yYQS771aclCOmKwrsubTP0j777ONimDg2B/t85whe2D3N8THBG8sZS454j81LTASX04mqo/H8888n9qhRo1xM//79nc++M5jg7YADDkhsJjJiz7e9N7bzXilshzd2PtutLkcAXkuwZ4bNG/Pnz09slvtWBGevL+CF+EBlXdDYc8wEb/YdxYR6CxcudD4r7GSCyTVr1iQ2yw3WCa+lqeib3xBC30bmGQBmV2c4QgghhBBCtBw5W53dDuBoAL1CCEsAXAXg6BDCeAARwCIAn27BMQohhBBCCFEVyi5+Y4znEPdNLTAWIYQQQgghWpSKan47EvPmzUtsu2kzwOt9bJ0eq39ryVo3dj5b55VbJyjyWLZsmfOxWtCtW7c2aQO+bi+XHj16JDarA2P1Ylu2bEnsnM3XOyK5dYe2Lm2PPfZwMbbmjx2b1fPaY+fUyeZqESy5Nbh2TKyesU+fPomdW/PM8r9WyKld3bRpk4uZPn16YrOaX9ZkYuXKlYk9ZswYF2Nrbl955RUX07NnT+fbsGGD8+Vg57j77vMSn7POOiuxa7m+l8GeI/YOtrWz9plhn2PPB5vDLXZOB/wagNVns+fW1gGz98PcuXOd75BDDkls1szloYceSuwJEya4GPac2Z06qi1eVLsvIYQQQghRGLT4FUIIIYQQhUGLXyGEEEIIURi0+BVCCCGEEIWh8II3VshuYU0KbIF4jogkd+NvG8c2qmZF8nZDdCa2yW20Ifz1Y8IWtvG3vcYsfwYNGlTRmKwgc/369S6ma9euzmfzrKMJUtoCKzjJEbcBfq7IEaHm3i87BnZs2ygF8CIcJqQcPXp0Yk+dOtXFMKEOuwa1Qs51f+aZZ5zPinqYyKh79+7OlyPAtjHs/TBkyBDne+GFFxKbNdlgzQZ69eqV2IsWLXIxtmEGE9zVMux9y667neuZuMvmFBPT5swJ7Pz22DnNuAAvlHvzzTddzLhx48oehwnAbQ6zJhvsOdu4cWNiS/AmhBBCCCFEhWjxK4QQQgghCoMWv0IIIYQQojBo8SuEEEIIIQpD4QVvtpMJ6+bGBCI2Lqf4nB2bFbvbOCY+YQXirNjcwkRbonKYMMDCBD977713RecbOHBgYs+cOdPFWPEC4HOIiQ6KABMZsWciRyhiRUzsuud2fSv3OZZDzGfHzc7P7r09FhPcDB06NLGffPJJF2NFt2xMHY3nn3/e+SZOnJjY7BowERXr2mXJES2zHLM+dq+YmG3fffdt0gZ8Z7qOJnhjzwwTd9prytYFlQpA7X1n7xAr4M8RxgPAmjVrEpuNkYm0V6xYUfbYVrjG8pcdm62Nqom++RVCCCGEEIVBi18hhBBCCFEYtPgVQgghhBCFoWzNbwhhIICfA+gDIAKYEmP8fgihB4BfAhgCYBGAj8YY/Y777Zyc+juGrYljtT0WVveVU/+T0wgD8D8Lq1N66623Kjqf4PePXU9bC8pqlyqtZ7Ibhs+ePdvFbNiwwflsTZ6tHe6o5NTAsvtqN/Zn2Ho6pg1g2DGwGjg7n+TWCeZoEdicl7M5/qhRoxKb1ROypkFM61DL2OdrwIABLsbOAd26dXMx7L7bOTunBphdc3aP2f2ysDrSZcuWJTarz7RNLjoaOc1qAK8pYHOCve/sXrFj22eZPaM58x07tv2c1UIBPF/te4WNyda/s6ZP7H3I6oerSc7KbxuAL8YYxwI4AsDnQghjAVwO4JEY40gAjzTYQgghhBBCtFvKLn5jjMtjjC82/HkzgJcB9AdwOoBbGsJuAfChlhqkEEIIIYQQ1WCnfucfQhgCYAKA5wD0iTEub/irFdheFsE+c1EIoT6EUL969epmDFV0NJQboimUH6IUyg1RCuWGyCF78RtC6ALg1wAujTEmG2PG7QUxtCgtxjglxlgXY6zr3bt3swYrOhbKDdEUyg9RCuWGKIVyQ+SQ1eQihNAJ2xe+t8YY725wrwwh9I0xLg8h9AWwqqUG2ZLY4m9WDM6oRCjHisFzzsfOxUQktridFdvnNGUQHHY92f3bvHlzYvfo0cPF5AgkGfvtt19iM6EAE8BY4QyLKQJMAMKEHMOHDy97LPu5Shta5DSwyBG4Mh/LTzZOmw9MlGKbXLDmO2xMlW7q316x3yayZ9nmBhObWVEcO1ZOgxAmNssZE8v7kSNHOt+cOXMSu1+/fi7GigBZ/rCmELUCy2t2b+w8y0Rbr7/+ekVjsGJI9mzbGPbssbyz77YcoSXgG2utX+/3POjbt29iz5o1q+z5Ab9WYe/R5lB2Zg7bZ8qbALwcY7yu0V/dC+D8hj+fD+C3VR2ZEEIIIYQQVSbn659JAM4DMCuEMKPBdwWAawHcGUK4EMBiAB9tmSEKIYQQQghRHcoufmOMTwEotfHrcdUdjhBCCCGEEC1Hhyn8q1ZjBlYHlUOltW45G8BXulF1ETacb0lsTSPbqJ7d402bEj1oVRtKjBgxIrFZLWFObR1rgFIEcptcsMYFFvsM5tYT28+xZzKnJpzlnh0DO07OvGTr1gFf85urYehoNb/252bXoWvXronNriermd59990TO6eOnB2bzf1WH7Bo0SIXM2nSJOf7wx/+kNjs2bB53tFqfnNhTUIstr41p1kF4J/lnJp/dhyWGzYX2f1jjS9szS+by2xOszGxPG/pd5TaGwshhBBCiMKgxa8QQgghhCgMWvwKIYQQQojCoMWvEEIIIYQoDB1G8JYj/mBYEQATIeSQs6k4KzTPKWxn5AjsJHhrHvZasY24Wb5YsUD37t2rNqaePXsmNstx5rPCuEqatHQEcgVvVtzFsAIe24AE8IIQIK/BiJ0DcoVkOc0x2Ab21vfWW2+5GPuzsGPnzIO1jt3In4lOe/XqldhLlixxMSMhDKwAACAASURBVKzZgJ0r2LWzQiAmeGOfs/n64osvupjTTjut7JjYz2sbbeQ056h12Nzfv3//xGbXasGCBYltRcwAf7bsHJAjOGVzDXuP2ZxmMTnNcVje5bxr2LyR23CsUor5BhRCCCGEEIVEi18hhBBCCFEYtPgVQgghhBCFQYtfIYQQQghRGDqM4K1a5HZEsQXpOV1ackV5OR2Rcru+lRuTKI0t3mciAIa9D3vttVfZz+Tmhh0D65rExANWgFOEbkuM3G5j++67b9kYK2xkQjLWpWjNmjWJzeYX68sdt41jcwLrVLhhw4bEZgItOyYm5mGCFxZXy6xatSqx2TW2IjErKAL4tRo0aFDZGHts1lWsUrEQmxd69OiR2EzAZLt/rVu3zsX07t27ojG1B1jHs6VLlzrf2LFjE5tdh/nz5yf2yJEjXQy7f3ZeZ3OCvTd9+vRxMbYDKfuc7coGcIGmzRf281rxMHs/sefDxlkxYXPRN79CCCGEEKIwaPErhBBCCCEKgxa/QgghhBCiMJSt+Q0hDATwcwB9AEQAU2KM3w8hXA3gUwBWN4ReEWO8v6UGWo6chhaMgQMHJjarPWG1nrYehdWx2Fo3FsN8tv6G1QTmNONgtVk5TS4qvZYdHVv7VgpbU1nNml/baIDlRk7ji6LU/NraOfYsM19Oje15552X2LZuFgD69u3rfPYZzGl6wZ7bnCYXbA5gdci2xrmurq7smHJqlYGW36y+tbE1k7beFQA2btxY9jisRtw2XWL3feXKlYnN6jpZfab9nLUBXuttc4jdTxvDmqTUMqzelDW1sbWyrFb4pJNOSmxWE5/TQIbF2GebPY/s3th3FMtNNpfYn5fV/Fqd0eGHH+5i2DXIafbVHHIEb9sAfDHG+GIIoSuAaSGEhxv+7j9ijP/ecsMTQgghhBCiepRd/MYYlwNY3vDnzSGElwFUV3YnhBBCCCFEK7BTNb8hhCEAJgB4rsF1cQhhZgjh5hBC9xKfuSiEUB9CqF+9ejULEQVFuSGaQvkhSqHcEKVQbogcshe/IYQuAH4N4NIY4yYA/wVgOIDx2P7N8PfY52KMU2KMdTHGulre509UH+WGaArlhyiFckOUQrkhcshqchFC6ITtC99bY4x3A0CMcWWjv78RwH0tMsIWxopUmFCBicuWL1+e2EwEYAu9K93snQlUmBBixIgRic02s3711VfLni9XfNXRscX7K1ascDFM9GAFBTnistwmBlYEwIQJ7F7ZuM2bN2edr9axz0nOcwoAa9euLXvsCy64oPKBdQByG+3Yph61zpw5cxKbNSnImetZ3tnnks0dEydOTOx58+a5GNYc44Mf/GBis3vFfFYEzgR+o0ePTuxx48a5mFqGNX1gPsvixYvLxrD3O4PN9Rb7TDLBHROu2WPnrlWssJLlj11TMeF4zrWsNmW/+Q3br+ZNAF6OMV7XyN9YxnwGgNnVH54QQgghhBDVI+eb30kAzgMwK4Qwo8F3BYBzQgjjsX37s0UAPt0iIxRCCCGEEKJK5Oz28BQA9jvvNtvTVwghhBBCiErIqvmtBSqtU7WbubNaJVajktNkwta/2AYFAB+j/VnYRtVsA2i7WT+rW5w0aRIfbJkxFRFby3fmmWe6GFZb17Nnz8Q+/vjjy54r95rbertRo0a5GLbR+KBBgxL70EMPzTpfrWNrJseOHetiBg8e7HyTJ08ue+ycOu2O/CxdfPHFzjd//nznO+yww1pjOK3GNddck9ishtLmBpt32XNqm40w3YaN+epXv1p6sE3Amg0wjjzyyIqOX0TsO5/V89r3NFtL5NTAsjWArSPPadAF+Lpcm2MArznee++9E9s2aQHyfpa20BmpvbEQQgghhCgMWvwKIYQQQojCoMWvEEIIIYQoDFr8CiGEEEKIwhByN9evyslCWA1gMYBeAGpx53ONu3kMjjHSljuNcgNoP+PdGWpxzED7GXfJ3AA0d7Qh7WXcOXNHexnrzqJxNw+9V9of7WncND9adfH7t5OGUB9jrCsf2b7QuFuHWhsvUJtjBmpv3LU23h1o3C1PLY21MRp361Br4wVqc8xAbYxbZQ9CCCGEEKIwaPErhBBCCCEKQ1stfqe00Xmbi8bdOtTaeIHaHDNQe+OutfHuQONueWpprI3RuFuHWhsvUJtjBmpg3G1S8yuEEEIIIURboLIHIYQQQghRGJq1+A0hfCCEMDeEMD+EcHm1BiWEEEIIIURLUHHZQwhhVwCvAjgBwBIALwA4J8Y4p9RnevXqFYcMGVLR+UTts2jRIqxZsyawv6uF3Ni2bZvz/eUvf3G+PfbYI7F32aXlfsHy7rvvZo1pzz33bLExVIOmcgOojfwQLUetzx2i5VBuiKaYNm3aGrbP727NOOZEAPNjjAsAIIRwB4DTAZRc/A4ZMgT19fXNOKWoZerqSm/7V2lu2H+8heDnQPYPPBZnsQvLjRs3upgFCxY43+jRoxO7S5cuZc9VKWxMixYtcr6DDjoosXN+fkal17IcTeUGoLmj6LTE3CE6BsoN0RQhhMXM35yvpPoDeKORvaTBZ098UQihPoRQv3r16macTnQ0lBuiKZQfohTKDVEK5YbIocUFbzHGKTHGuhhjXe/eJbuXigKi3BBNofwQpVBuiFIoN0QOzSl7WApgYCN7QINPiDaj0l/LX3nllc63ZcuWxLa1vACwfPly59u8eXPZ82/dujWx2a/u3n77befr3LlzYs+cOdPF7LPPPs43YsSIxF6/fr2LOeussxL78MMPdzE5ZSXVKIMQQgghWormfPP7AoCRIYShIYTOAM4GcG91hiWEEEIIIUT1qfib3xjjthDCxQAeBLArgJtjjC9VbWRCCCGEEEJUmeaUPSDGeD+A+6s0FiGEEEIIIVqUZi1+xf/Bak1zaiFz6iNz92KuVq0l275r2LBhic1UtL169WqR8ewMuee87rrrEnvdunUupn//dPMStn/u4MGDnW/Dhg2JvWzZMhdzwQUXJPZpp53mYv7u7/6u7JgGDBjgYpjIw+5RvPfee7uYW265JbEXLlzoYs4++2znU82vEEKIWkLtjYUQQgghRGHQ4lcIIYQQQhQGLX6FEEIIIURh0OJXCCGEEEIUBgneWpFKhUDVFBDNnTs3sWfPnu1iXnrJ71g3ffr0xGYivF/96leJvdtuLZ9eOWIr1tDhT3/6U2IPGTLExdhGFOzYu+zi//04cODAxLbHAYD58+cn9rPPPutiWLOK/fbbr2zMu+++63x27J06dXIxVkz34osvupiPfvSjzmevwV//+teyMUJUk/YuusxtvpPzc9jnK1dInXPsnHFW2khIVJdK7wN7H73yyiuJffDBB1c0Bjamas39OcL/nclDvZGEEEIIIURh0OJXCCGEEEIUBi1+hRBCCCFEYVDNr6HSOppKG1gwHnvsscQ+8MADXcy0adOc7+qrr05s25gCAJ566qnEPuSQQ1zMxIkTne/6669PbFsf2lbk1BNNnTq17OfefPNNF7Pnnnsm9jvvvJM1pk2bNiV2v379XMyqVasS+84773QxdXV1zrdx48bE/vOf/+xi2DWxNb6sLtfmPmvqMW/ePOcbNWqU8wnRmpSba5cvX+58VpPAnqVBgwY1b2AN5L4LcuIqraGs9D1WSYxoeXLWKqx5009/+lPn22uvvZq0AaBz587OZ5s8Vdq0K6eeNyfv2Xut5PGyI4UQQgghhKhxtPgVQgghhBCFQYtfIYQQQghRGJpV8xtCWARgM4B3AWyLMfoiRSGEEEIIIdoJ1RC8HRNjXFOF43R4rMgJ4A0J7r///sR+8sknXczatWud75JLLknsyZMnu5gBAwYk9uuvv+5ibAMIwItDWCF9jx49nK89wJo12AJ7JnizBf65Yshdd901sZlQbo899kjsLl26uJgtW7aU/Rw7NhMGWPHehg0bXIwV/LCfjTVAsYI3NbQQrUmM0T0HTzzxRGLffvvt7nNW7Mvytlu3bs43dOjQxGbPkm2sw0ShbA7v2bOn81msoJYJkdjPYt81q1evdjFsDrefY+ezsLlj27ZtzmfvGxMdrlixIrHPOuusxN4ZkVNHIkckxsTe9913n/PZnGbvnrfeesv59t9//8Q+55xzXMzee++d2JVuDsAE2DnNm0qht5QQQgghhCgMzV38RgAPhRCmhRAuqsaAhBBCCCGEaCmau/idHGM8BMBJAD4XQjjKBoQQLgoh1IcQ6tmvWURxUW6IplB+iFI0zo01a1R1J/4PzRsih2YtfmOMSxv+vwrAPQBcd4QY45QYY12Msa53797NOZ3oYCg3RFMoP0QpGudGr1692no4oh2heUPkULHgLYSwN4BdYoybG/58IoB/rdrI2ohKu9cw4ZHthsXEBKyTymc+85nE/va3v+1irHAN8MXmrEDd/ny2YB0AZs2a5XxWQGIFVABwyimnOF97gHUlswI+dv+s+MKKzQBeYG+PzYQJOSKSrVu3Op8V09lzlRqTFTAwQYwVFLBxs05ZQrQlb731Fp5//vnE9+yzzyb2Nddc4z73wgsvJPY999zjYtg8N2HChMRm84t9nu38CQB9+vRxvjfeeCOx2TeX9p3Rt29fF8PmcPuPBPY5ex3Z+Zgoz16nhx56yMWwb+gPPfTQxB4/fryLsQI/OwcxIVQRsO8CBsu7OXPmOJ99/zER4Yc+9CHns8/Z17/+dRczadKkxB43bpyLYeuZuXPnJvYzzzzjYo46Ki02OOCAA1xMKZqz20MfAPc0LKZ2A3BbjPGBZhxPCCGEEEKIFqXixW+McQGAg6s4FiGEEEIIIVoUbXUmhBBCCCEKQzWaXHQoWK1LzqbMrO7I1l7Onz/fxTzwgK8UsfVpM2fOdDEHHnig81ns5tIMVhfMarpsM4zvf//7LqauLm3wx+qJWxrWNKRr167Ot3HjxsRmNbdLlixJ7EGDBmWdz+YQi7GwTcUZNs8qbSjBzmc33R88eLCLYfWNQrQlnTp1wn777ed8jWF1jrZ50L777utimO/RRx9N7GOPPdbF2IZGP/rRj1zMGWec4XwLFixIbPZ8f+ITn0hsVhfMmvbYmtu3337bxcyePdv5bF0lq1W2NcasCRLTItjru2zZMhdjr/fHP/7xxGa6h46I1WDkNCF66qmnXAxr3GLfhzNmzHAxzHf00UcnNmvmYo/NxrR06VLns+9k1rTr+uuvT+zLLrvMxZRC3/wKIYQQQojCoMWvEEIIIYQoDFr8CiGEEEKIwqDFrxBCCCGEKAzFqBTfCXLEbYycovvf//73znfBBRc43ze/+c2yx6oWmzdvdj67qTgAHHLIIYl98cUXuxi7UbZtEsHEhNWGCT2YqM8KSdjn7GbqY8aMcTHsZ7K+bdu2uRi7QTkTxbG8Y40nyh0bAHbffffEthv8A0CXLl0Sm4k4169fX/b8RSXn3gD+vuaIbFkusM9VKoDMEdNUis1tNsbmnG/r1q147bXXEp8V6C5atMh97uCD0506X331VRdjBWiAF/4cf/zxLsY2qxg7dqyLYUI1K1IePny4i7HYZxsAhgwZ4nxWlGavEcBFcBbWUc8KCpnY2TYHAYBXXnklsZkYygr1cprx1BLVHP8XvvCFxF64cGHW5+z7j4kTWZ7df//9if3HP/7Rxdj3kW16AQCjR492PjuGa6+91sXYnJ4yZYqLKYW++RVCCCGEEIVBi18hhBBCCFEYtPgVQgghhBCFQTW/hkprz1g9zMiRIxP7W9/6VtaxbO0sqyfOGSerJbKfY3VnbGN3W4v2/ve/38XYjc1trRarfa02rG7PXk/AXxtW82try9h9YMe2NY2sBrfSPLOfY/WT7L7bsbOaLnv/+vbt62JYvd+GDRsSm+VPEahmnWzOsSqt7/3tb3/rfJdcckliL168uKJjM1j+V5Ndd90V3bt3T3y2yQTLZTtXsBrqFStWOF+/fv0Sm12r22+/PbGPOOIIF8Nqbq224sEHH3QxttGM/QwAPPvss85nm1U88sgjLoY9u88991xis/tp67pZrbS9J4DXhdhmJezY9j1S6zW/1Zw3evfundh77LGHi2FNn2ytN9N7MC3QnnvumdhMQ2TzhWmfHnroIeez9501QDnnnHOcLxd98yuEEEIIIQqDFr9CCCGEEKIwaPErhBBCCCEKQ9nFbwjh5hDCqhDC7Ea+HiGEh0MI8xr+372pYwghhBBCCNEeyBG8/QzA9QB+3sh3OYBHYozXhhAub7C/Uv3h1TZWQFFp04Ic4VqlWFEa4JsdsDEwcYgtmmeF9S3NypUrnY9dPytQZMX8VjzAxG1MBGevDTt/zsb/OUK5zp07lz0/4AUMTKBpN+9nG/Ozn8UKcA477DAXU1Rynt1KhWtPPPGE81lx0o033uhi2PNtRVysic3111+/s0ME4HP95ptvdjGf+tSnKjo2sD3frXBq2LBhiX3iiSe6z919992JzRo8jBs3zvm6deuW2LahBQB87WtfS+z//d//dTFMAPbAAw8k9tFHH+1ijjnmmMS2zXgA4O///u+dzza2sbkCAJ/97Ged79RTT03sHKEea1bBfl6LbTwC+HnIivJaWlBZS9iGTqx5EvPts88+iW3ffaV8L7/8cmKze2HnQHZ+9v6171Y2TzJhZS5lZ90Y4xMA1hn36QBuafjzLQA+VPEIhBBCCCGEaCUqrfntE2Pc8c/NFQD6VGk8QgghhBBCtBjNFrzF7d9pl/zdfQjhohBCfQihnu0pK4qLckM0hfJDlKJxbmzcuLGthyPaEZo3RA6VLn5XhhD6AkDD/0sW88QYp8QY62KMdaxmRBQX5YZoCuWHKEXj3LA1uKLYaN4QOVTa4e1eAOcDuLbh/75lkMgSsjDhmv0cEzAxbGF5jiiOdWS54YYbnO/MM89M7DPOOMPF7LXXXoltxVjV7GRTCluAD3Chmu18s3TpUhdTV1eX2Ox+5nStYwX+9lqw4+TcdzamHEFDjhCC3S82pjlz5iR2UQRvOc9bTs6vXbvW+X7zm98kNhO33XHHHc73nve8J7FHjRrlYlgXL/vc3HrrrS6mUsGbHTsTQzVH8PbOO++459d2ImQdz+w3xqzrIftW2XZ9mz59uos57rjjEpsJY60ADfCd4Zgw9aabbkpsJkD7/Oc/73wnnXRSYt97770u5sADD3S+LVu2JPbPf/5zF2O/YbV5CABbt251PiuWtcJFABg/fnyTMbnvx/YKE8Uyn53rWRc2ez3tOxngXd/sPWYxTChrBfNWOAt4MRu7x7ZDI+DnxcmTJ7uY9evXJzZ7FkqRs9XZ7QCeBTAqhLAkhHAhti96TwghzANwfIMthBBCCCFEu6bsN78xxlLNk48r4RdCCCGEEKJdog5vQgghhBCiMFRa81uTtGSziJaE1XXm1Dnl/Gx9+vhd6ljN5jPPPJPYZ511loux9TYHHXTQTo+nuSxZssT52Ob1tu7Rbg4O+Ho7VjucU9edc69YbrIaxBxYnaCtt+vRo4eLsTVkbNxvvvmm87FN/tsLOXW5rP45Z+P8nHy2tXQA8K1vfSuxf/zjH7uYvffeO7GHDx/uYljjBnuf2f1i9Zjz589P7Jz6XvbM2CYNAHD22WcnNmtEY3No4MCBZc+/g7322guHHnpo4rOb37OGLfYcDz/8sIux9b2Ab2DBmkxcc801ic1qKFldta1V/va3v+1ibLMI9rw/+uijzveRj3wksa+++moXw+6f/fkmTJjgYuxcf9ddd7kYqw0AgCOOOCKxWR2rrY22n6n1mt9cbYXlsccecz77DmY1uOy5tfMdq3Vnc5nNa/autfnJ7jEbk83zq666ysXY3GBzeSn0za8QQgghhCgMWvwKIYQQQojCoMWvEEIIIYQoDFr8CiGEEEKIwlAowVstiNtyyRFaMexm8CNHjnQxbDPpF198MbGnTZvmYqzYxnZeYhu9VxvWMIBtrm5hY7OiIwYrsLdihUoFb0wYYO87E9LkfI5tWG5hAj+7YTnAG4S0BTmbxbM5IEfcxrCNIWyDAoALx6zI1Ap4AC92ZHnNrnvXrl0T2zY3AXiTiSFDhiT2T37yExfzz//8z4nNNtCfNGmS89l8ZA1dmGgrl1122QV77rln4rvzzjsTe+LEie5zF154YWJbgQ3gN/EHgL59+yb2f//3f7sY+5wwsddRRx3lfIMGDUrsT3/60y7GCuXY/HLkkUc632uvvZbYTMC0bt0657NzB7smtjlG7rX88Ic/nNg//OEPXYxtxFTrAjcL+3ly5iTWwMY+k+zdx85n7zGbW+wzBvhngZ3PitlYYy3WiW/YsGGJzYTB116btpgYOnSoiymFvvkVQgghhBCFQYtfIYQQQghRGLT4FUIIIYQQhaFQNb+1Sk6NDuOWW25xvtWrVyf2Jz/5SRfDNs/u3r17YtfV1bkYWy9WaS1lc/jzn//sfKwW1Maxjfdtze/ObKDdmJwmJezYrMbKwj7H6lptLRar17R1l+xaspquSq9LtWE/d6V1/r/61a8S+9/+7d9cjG3MMGLECBfz3ve+1/lsPe+iRYvKjoc9Syyv7L1gnxswYIDzrV+/vuwYTjrppMS+6aabyn4GAG688cbE/sY3vuFiRo8endh33HFHYrM64R1s3boVCxcuTHy2npZdhxkzZiT2cccd52JY3fvTTz+d2IcccoiLsU10bP0iAAwePNj57Jxt68oBXyfLaij/8Ic/OJ/Nu8MPP9zFsDlnv/32S2ymhbjvvvsS29YAA8DXv/5153vppZcSm+kV7L2zjUfYPdoZcrQCOTGAH2vO/FOpfoc1o7I1/0zbwWq97ThtLS/An0Fbz5tTu9+5c2fnY8+nPd/UqVNdjP15dwZ98yuEEEIIIQqDFr9CCCGEEKIwaPErhBBCCCEKQ9nFbwjh5hDCqhDC7Ea+q0MIS0MIMxr+O7llhymEEEIIIUTzyRG8/QzA9QB+bvz/EWP896qPSDhYQfyGDRuczwpJWIH6/vvvn9i///3vXQzbPNuKClizg9ZoYmFpSgizgzfffNP5rGjife97n4uxIj/7GYAX+Nv7ldMIw4pRgDwhBxOosGtiRQ79+vVzMbYpiRVHAlx8ae87+3lbQvxoBWePPvqoi5k9e3ZiM7HHkiVLnM8+X7YxBeAbxLBN/N9++23nsyIRJoqx94KNmwlubB7lCIgAL4xhoiYrorINGQA+L1ixFxOI2c/dfPPNic2afOygc+fOTjxmrzvLdyvKssI8FgMABx98cGJfdtllLubYY49N7OXLl7uYu+++2/lscwgmeLPXiolXWbOIT3ziE4nN8n7BggXO16NHj8S2zTIA4Nxzz01slq+/+MUvnM/Ou0wgakWAtiELE1A1hZ3D2Pu1NRtizZs3z/ms4BPw8xt7Rm2es/vA3it2DmfNcdh7xT5n7F1bSYMlwAuu2eduu+22xGZzSynKfvMbY3wCgG/7IoQQQgghRI3RnJrfi0MIMxvKIrqXDxdCCCGEEKJtqXTx+18AhgMYD2A5gO+VCgwhXBRCqA8h1LNfo4riotwQTaH8EKVonBt2f3FRbDRviBwqWvzGGFfGGN+NMf4VwI0AJjYROyXGWBdjrOvdu3el4xQdEOWGaArlhyhF49ywNami2GjeEDlUpFAKIfSNMe6o4D8DwOym4qtNpR3PWhs2TgsTB1nRihVBAMCXvvQl5xszZkxis85Rl19+eWLnFvYvXbo0sZctW+ZimFCupWEF/RZWqG+/LWLXIUdwxsRD9li53YFysMIElmNM5GSFeUyYYLtS1dfXu5j3v//9zme7vjGRV3M68QDbBWhWIGTFDuxnsteHCQRZ1zor+GDXeePGjYnNcsEK1wDAvpBZftr5zIrkAJ5D9tqzY7OOYPb47JrYRSYTuNpuYICfz9j5m/vtrf05jz/++MRm3Qrvv//+xH788cddTP/+/Z3PPktjx451MUyoZmHvrJNPTjdOYtfKCm/32GMPFzNxov8+atKkSYnNnlP2DNk4Ni9aUaMVmgK8I6DtrPmxj33MxZx33nmJbbsR7myHyUrWCix/7PMPAIsXL05sJnS89dZbE/uFF15wMUzEaH9OJkqz72XWdZIJ5ex71IqJAS4stPOG7QIJ+Jz6zW9+42LY3GkF50xc/sgjjzhfLmUXvyGE2wEcDaBXCGEJgKsAHB1CGA8gAlgE4NMVj0AIIYQQQohWouziN8Z4DnHnNXUXQgghhBCiHdH+agWEEEIIIYRoIVq/K0EVyK3ZyamrbMnNrO04Wd0ga25g67yuu+46F8Nqa5588snE/slPfpI1zhzsdcr9WVoaW4vF6thYHtgmBuPGjXMxtg6K1TiyejN731mO2c+xmEpr29mxbA0nq32dMGFCYr/yyisuhtV92Z+F1Ys2t+a3W7durh7Sbmj+7LPPus9Nnz49sVkdPGugYOsqWR11TnMPVjtn6wDZPbU1rOyasnreHJ0BuxfWx+rr7PPNxp1TT2hrywGfj6effnpi20YHjXnnnXdcQxZbL8jGamuYjzjiCBfDxjplypTEtnoIwDdFYc/bQw895Hw2p1jTB9t4g9WDX3XVVc739NNPJzZrymLrggGvOZkzZ46LsfXTH/7wh13M0UcfXfbYLH9tU4tK9RI7sI08vvrVr7oY2wDE1iYDeY2JWHMcO4cywSbLF3tt2HN80EEHJfaPf/xjF2Pr4QFfc8/qyFkzDgubg+27dvjw4S6G1S/bdRCrsX711VfLjqkU+uZXCCGEEEIUBi1+hRBCCCFEYdDiVwghhBBCFAYtfoUQQgghRGGoScFbLi0pZrOwInx7/lyh3jXXXJPYAwcOdDGsAcGPfvSjrONXgv1Z2Kb0bKPqlsYKDJhQxxbcA16Ec+qpp7qYF198MbGZwCFHYMSESfb8bNzsc9bHrjnbkN2OnTUHsU1SNm3a5GLYJvhWwJHTeKQa9OvXL7HPOcfvymh97JraTfMBL3hZuHChCJXwqwAAEYNJREFUi7Eb2rOfm+WH9bF5wQplWLMM1lDCCke6dOniYpjP5h8TNlqYwC9HjMQ28Lf5aecb9nzsYNddd3U/k7039n4CXtw1aNAgF8NEn7bxBRPwvPbaa4nNBESnnHKK81lh5cEHH+xirEiM5QH7ee2xe/Xq5WJYntu4Y4891sXYZ+gPf/iDi7FCPQA488wzE3vmzJkuxooZrdi73HvHPm+f+tSnEtveK8ALD9ncz0RaFjYX22PnioHtdZg7d66LsWsH9qx94xvfcD6b++xzH/nIR5zP5j4ToFlBKBPzsbnTzi/sHuy///7Ol4u++RVCCCGEEIVBi18hhBBCCFEYtPgVQgghhBCFQYtfIYQQQghRGGpS8JYjLgN8EbXtGAJ44daoUaMqGlOl4robbrjB+WxhtxVeAcBNN91U0flyBFpMgGM/ZztgtRU5IjuWL/YasyJ8K6br2bOni8npwmaPA3jREfs5mAiAdfqyMAGMzU8rCAJ85x/bJavU+e3PwrrsNZcQghMNWQEPE+NVKsB6z3vek9hsXsgRsObMVSzGCvNYnjHBmf0c60zHRDi2exK7lvbYuR0P7flYFzj7PFrBH3uGGmPvhT3nE0884T5jxWzsGjMxpBVRDhs2zMU8/vjjiT1+/HgXc9hhhzmfHTfr7mmfAyZcY/fPds2zHRIBLoZ64YUXEvsrX/mKi7HPy7/+67+6GCams3nHxGdWKGfnl6beaZs2bcIjjzyS+F5++eXEZqJCe99ZHuS8A9nz99JLLyU2E0yOHDnS+awAecCAAS7mxBNPTGzWcY1137OdL5nYeerUqc537733Jja7FzZf2fshRyTN5hs7L7Bxl0Lf/AohhBBCiMKgxa8QQgghhCgMZRe/IYSBIYTHQghzQggvhRD+qcHfI4TwcAhhXsP//e9IhRBCCCGEaEfk1PxuA/DFGOOLIYSuAKaFEB4G8AkAj8QYrw0hXA7gcgC+GKgFyK2vXbZsWWIvX77cxdh6RVZbxjZXrgRWc8xq0WxNzO9+97uqnJ+Rey1tHKvNagtsDSrbnJ/VGNprzOo+bb2WzSeA1wHbur01a9a4GFvTuHbtWhfDfha7sbrd+Bzw9VuAr99luWjvsd2cHAAmT57sfLZeurWaXNjrk9OYgcGe+ZxGFPZzOU1JGKxOztYB5zbIscdizzfLdZbHFlvPy+p7c/QCLMbeO7vxf1Nz8G677YYePXokPvvMjxs3zn3Ozh2srvOMM85wPtscw9aQAsBRRx2V2LaBDMCfQfvssHpQW2vK6ntZbb59/7EGHqz+1dYUv/HGGy7GNmEZMWKEi2H5Ymt+mfbBvqPtHNiU7mO33XZD7969E5+t32fzsz0na6bA8sXev5y5n+WGvS6AfybYe80+R+9973tdjM1fAJg9e3Zis/cKazRj5w0WY2t1WcMXlq/2vjJthH1Hs3dWKcrOqDHG5THGFxv+vBnAywD6AzgdwC0NYbcA+FD2WYUQQgghhGgDdqrmN4QwBMAEAM8B6BNj3PFPyRUA+pT4zEUhhPoQQj3714QoLsoN0RTKD1GKxrnBfmsiikvj3GCt7YUAdmLxG0LoAuDXAC6NMSb7ScTt30fTPYVijFNijHUxxjr76wdRbJQboimUH6IUjXMjp2RDFIfGubHvvvu29XBEOyVr8RtC6ITtC99bY4x3N7hXhhD6Nvx9XwCrWmaIQgghhBBCVIeygrewXTFxE4CXY4yNd9y+F8D5AK5t+P9vc05oi5YraQ6R2+TCbj7ONiNvTf7xH//R+err653vqaeearExWLFJThMAwF/fOXPmVG1MzcEKipgojf1a1Bbhs1+P2Z+ZFeUzIY4dExNUrVqV/lvRbiQP+A3LAS+8YD8bE1DZzb+Z6Mh+uzpw4EAXM3bsWOebO3duYpdrSNDeqFTQyoQbovXZunWrE3naJkCDBg1yn7PCKdYc5sc//rHzWaFYXV2di5k/f35iv/766y7GNp0AgOeeey6xc8RYDPY5+5yyn/dPf/qT81mhFfucbYAwffp0F2NFcYAXwbFGBvZ6z5gxI7FZ05YddOrUyc1rdl4/4IAD3OesiHDJkiUuhl2Hfv36JTabQ+38mNvMyI4pZ+5nQr3nn3/e+WxOMcEiO5YVjtufH/DzK7vHbA6295U9Q3b9wvKuFDm7PUwCcB6AWSGEHVl3BbYveu8MIVwIYDGAj2afVQghhBBCiDag7OI3xvgUgFJfzx5X3eEIIYQQQgjRcqjDmxBCCCGEKAw5ZQ9VpZIa30qPYetB/uEf/sHF2GYN3/nOd1yM3bA8F1svdtttt7mYb37zm87Xt2/fis7XkthaIluz2lbYmiO28TjDbo795JNPuhhb08VqjlhjBbt5PduE3da6DRgwwMWw2j57bNZEganfbS0fi3nggQcSm9XSsTozu7F5e8kNUQx23XVXt+H/aaedltgsl+0zwTboP/roo53P1nqzJhP2OWV1jk8//bTz2WY07NgWW5ML8GYVttaUNatg2KYa8+bNczF2PhsyZIiLYXOHbZDDmnpYn7XZfdtBp06d0L9//8R37rnnJvZ1110Hy8iRIxP7Pe95j4thNf/2frF63rfeeiuxWW6yed02p2F1snZtZOvaAa59su8olq/2nQl4nQhrzmGvk83xUj77bmXjtg1m7L1uCn3zK4QQQgghCoMWv0IIIYQQojBo8SuEEEIIIQqDFr9CCCGEEKIwtKrgbcuWLW6jbVu0bQvgAS/8YYXeTFRki7ZZgfrs2bMT+6qrrnIx9913n/NZgYU9DjvWGWec4WK+/OUvO19rkisetII3W3zfVqxbty6xx48fXzYG8JvAs825rRCBCSus4A7w+ckaidjPsYL/HNEBi2HHskJAdv/sxupMSDNr1izns8KEaohahcjl3XffdUIjKxpmYh07rx9xxBEu5v3vf7/zWbHoo48+6mKs0IjNE2xe+tjHPpbYzzzzjIuxDT1Yw5rBgwc7n231axtxAPwdaYVyPXr0cDFW4McEYuxzv/nNbxL71FNPdTF2Xlq6dGlis3vbFBdeeGFiH3rooS7mmmuuSWzW1Ik1TrHX2K4TAC8OZONngjf7uZxmX+zYrIGGFeExoV5OQywWY3ORiTiZGNLm9cKFC13MkUcemdjseS2FvvkVQgghhBCFQYtfIYQQQghRGLT4FUIIIYQQhUGLXyGEEEIIURhaVfD2zjvvYMmSJYnPCuBWrlzpPme7hjDBW69evZzPFkwzEcDnP//5xD7ssMNczLRp05zPihxmzJjhYk4++eTE/t73vudimFDPFrszUVNrY8VetoNSW2GL91mBPxNxWAEKE2lZsQITCjCBZo4wwHZBGzNmTNnPsGMz4RoTS1hxBvt5rUiHCVRYLtprwJ5PIVqK3Xff3XV/ss83y9vzzz8/sa2gCACmT5/ufLYbI+vOOHHixMS+8847XYztIAkAy5cvT2wroAKACRMmlD0OmwftNRk6dKiLYe9I+85m19KKAK0oD+Ddt8aNG5fYTNRkO8qdd955ZcfTGDtn2rmPiaTvuuuuxH7llVdczCWXXOJ8VvjORI1WPM7yzooMAb9WYO8Zm4tsnj/ggAOcz75HWHdRNk4LO599HzARoL0mgF9j2K57AO/6lou++RVCCCGEEIVBi18hhBBCCFEYyi5+QwgDQwiPhRDmhBBeCiH8U4P/6hDC0hDCjIb/Ti53LCGEEEIIIdqSnGLSbQC+GGN8MYTQFcC0EMLDDX/3HzHGf889WdeuXXHcccclPmvnYJsPAH4TfwDYtGlTYq9YscLF2LoZW18EAI899pjz2Tpk1sDC+mxNZSnaQ42vxTYyuOKKK1zMRRdd1FrD+Ru2Voltls1qhVavXp3YrNbcbsbNapVs7S7ga7PY52wjClZPzDbGt9gcB/jzYWuxWG3WggULEpvV7rJGH7Y2eODAgXywQrQAnTp1ck0trM045phjWmpIbXquWsLqYiqB6S4aU42mO6NHj3a+hx56qOzn7HsGADZs2JDYrJkQe6/sv//+ic3WCUynIThlV1kxxuUAljf8eXMI4WUAvnJdCCGEEEKIds5O1fyGEIYAmADguQbXxSGEmSGEm0MIvqfq9s9cFEKoDyHUs38FieKi3BBNofwQpVBuiFIoN0QO2YvfEEIXAL8GcGmMcROA/wIwHMB4bP9m2O/jBSDGOCXGWBdjrOvdu3cVhiw6CsoN0RTKD1EK5YYohXJD5JC1+A0hdML2he+tMca7ASDGuDLG+G6M8a8AbgQwsaljCCGEEEII0daUrfkN26vFbwLwcozxukb+vg31wABwBoDZ7PMtAStwZ75+/folNitat0iYwLGbrV911VVtNJKU4cOHJzYTFT777LPON2XKlMRmzUbsBuV2I3cA2LJli/PNmTMnse+44w4XY0V47Pz2OABgv8lgm6Gfeuqpzmc3KGebr/fp0yexragTAB5//HHnsyKLESNGuBghhCga7JvnnG+jrbhNVJ+cbQUmATgPwKwQwo42ZlcAOCeEMB5ABLAIwKdbZIRCCCGEEEJUiZzdHp4CwPYKub/6wxFCCCGEEKLlUIc3IYQQQghRGNpfNwVRE3z+859v6yEA8LVRV199tYthdaqHHXZYYrOa20qZPHlyYrdF849qwBq3XHnllc53wgknJPYuu+jf1EIIIdoveksJIYQQQojCoMWvEEIIIYQoDFr8CiGEEEKIwqDFrxBCCCGEKAwhxth6JwthNYDFAHoBWNNqJ64eGnfzGBxjpDt8N8oNoP2Md2eoxTED7WfcJXMD0NzRhrSXcefMHe1lrDuLxt089F5pf7SncdP8aNXF799OGkJ9jLGu1U/cTDTu1qHWxgvU5piB2ht3rY13Bxp3y1NLY22Mxt061Np4gdocM1Ab41bZgxBCCCGEKAxa/AohhBBCiMLQVovfKW103uaicbcOtTZeoDbHDNTeuGttvDvQuFueWhprYzTu1qHWxgvU5piBGhh3m9T8CiGEEEII0Rao7EEIIYQQQhSGVl/8hhA+EEKYG0KYH0K4vLXPn0sI4eYQwqoQwuxGvh4hhIdDCPMa/t+9LcdoCSEMDCE8FkKYE0J4KYTwTw3+dj3uHSg3WpZazo9ayQ2gNvOjlnMDqJ38UG60PsqNlqVW86NVF78hhF0B3ADgJABjAZwTQhjbmmPYCX4G4APGdzmAR2KMIwE80mC3J7YB+GKMcSyAIwB8ruH6tvdxKzdah5rMjxrLDaA286MmcwOoufz4GZQbrYZyo1WozfyIMbbafwCOBPBgI/urAL7ammPYyfEOATC7kT0XQN+GP/cFMLetx1hm/L8FcEItjFu5ofzoKLnREfKjVnKjFvNDuaHc6Ki5UUv50dplD/0BvNHIXtLgqxX6xBiXN/x5BYA+bTmYpgghDAEwAcBzqI1xKzdakRrLj1rPDaD9X+O/UWO5AdR+ftTCNQag3GgDauEa/41ayg8J3iokbv/nTLvcKiOE0AXArwFcGmPc1Pjv2vO4Owrt/RorP9qW9nyNlRttS3u+xsqNtqW9X+Nay4/WXvwuBTCwkT2gwVcrrAwh9AWAhv+vauPxOEIInbA9AW+NMd7d4G7344Zyo1Wo0fyo9dwA2v81rtXcAGo/P9r9NVZutBm1cI1rMj9ae/H7AoCRIYShIYTOAM4GcG8rj6E53Avg/IY/n4/ttS3thhBCAHATgJdjjNc1+qt2Pe4GlBstTA3nR63nBtDOr3EN5wZQ+/nRrq+xcqNNaffXuGbzow2KoU8G8CqA1wBc2dZFz02M83YAywG8g+11QhcC6IntqsV5AP4AoEdbj9OMeTK2/2phJoAZDf+d3N7HrdxQfnSU3KjV/Kjl3Kil/FBuKDc6Um7Ucn6ow5sQQgghhCgMErwJIYQQQojCoMWvEEIIIYQoDFr8CiGEEEKIwqDFrxBCCCGEKAxa/AohhBBCiMKgxa8QQgghhCgMWvwKIYQQQojCoMWvEEIIIYQoDP8fKT/bHup/gesAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItxO79Yy4mjC"
      },
      "source": [
        "# Question 2, 3, 4, 5, 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG4S2wCFsftQ"
      },
      "source": [
        "# Mini-Batch Gradient Descent with cross entropy loss and wandb log \r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "                # loss function with cross entropy\r\n",
        "                L = (np.sum( -np.log(mini_batch_y_bar.T[ np.arange(mini_batch_y_bar.shape[1]) , mini_batch_Y ] ) ) / m)\r\n",
        "                self.loss_list.append(L)\r\n",
        "    \r\n",
        "                \r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = ( - np.subtract(one_hot_vector.T , mini_batch_y_bar) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "\r\n",
        "            wandb.log( { \"epoch\" : epoch } )\r\n",
        "            wandb.log( { \"training loss\" : self.cal_loss(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation loss\" : self.cal_loss(validate_X, validate_Y) } )\r\n",
        "            wandb.log( { \"training accuracy\" : self.accuracy(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation accuracy\" : self.accuracy(validate_X, validate_Y) } )\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "\r\n",
        "        L = (np.sum( -np.log(y_bar.T[ np.arange(y_bar.shape[1]) , Y ] ) ) / Y.shape[0] )\r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3j6_LFTAydy"
      },
      "source": [
        "# sweep config\r\n",
        "sweep_config = {\r\n",
        "    'method' : 'bayes',\r\n",
        "    'metric' : {\r\n",
        "        'name' : 'accuracy',\r\n",
        "        'goal' : 'maximize'\r\n",
        "    },\r\n",
        "    'parameters' : {\r\n",
        "        'epochs' : {\r\n",
        "            'values' : [5, 10]\r\n",
        "        },\r\n",
        "        'hidden_layers' : {\r\n",
        "            'values' : [3, 4, 5]\r\n",
        "        },\r\n",
        "        'num_nuerons' : {\r\n",
        "            'values' : [32, 64, 128]\r\n",
        "        },\r\n",
        "        'alpha' : {\r\n",
        "            'values' : [0, 0.0005, 0.5]\r\n",
        "        },\r\n",
        "        'eta' : {\r\n",
        "            'values' : [1e-3, 1e-4]\r\n",
        "        },\r\n",
        "        'optimizer' : {\r\n",
        "            'values' : [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]\r\n",
        "        },\r\n",
        "        'batch_size' : {\r\n",
        "            'values' : [16, 32, 64]\r\n",
        "        },\r\n",
        "        'intialisation' : {\r\n",
        "            'values' : [\"random\", \"Xavier\"]\r\n",
        "        },\r\n",
        "        'activation' : {\r\n",
        "            'values' : [\"sigmoid\", \"tanh\", \"relu\"]\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzHDn3C6sFt0",
        "outputId": "59d5aa21-d85e-474e-f3b5-3e4a6125cb6f"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config,  entity = \"cs20s002\", project = \"DL_assignment_1\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: f3aad59d\n",
            "Sweep URL: https://wandb.ai/cs20s002/DL_assignment_1/sweeps/f3aad59d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvjoF4wNE3ej"
      },
      "source": [
        "def train():\r\n",
        "\r\n",
        "    config_defaults = {\r\n",
        "        'epochs' : 5,\r\n",
        "        'hidden_layers' : 3,\r\n",
        "        'num_nuerons' : 128,\r\n",
        "        'alpha' : 0.0005, \r\n",
        "        'eta' : 1e-3,\r\n",
        "        'optimizer' : \"adam\",\r\n",
        "        'batch_size' : 128,\r\n",
        "        'intialisation' : \"random\",\r\n",
        "        'activation' : \"relu\",\r\n",
        "        'beta1' : 0.9,\r\n",
        "        'beta2' : 0.999,\r\n",
        "        'gamma' : 0.9,\r\n",
        "        'epsilon': 1e-8,\r\n",
        "    }\r\n",
        "\r\n",
        "    # wandb intialisation\r\n",
        "    wandb.init(config = config_defaults, project=\"DL_assignment_1\" )\r\n",
        "    \r\n",
        "    config = wandb.config\r\n",
        "    \r\n",
        "    exp_name = \"hl_\" + str(config.hidden_layers) + \"_nn_\"+ str(config.num_nuerons) +  \"_ac_\" + str(config.activation) + \"_op_\" + str(config.optimizer) + \"_bs_\" + str(config.batch_size) \r\n",
        "    \r\n",
        "    wandb.run.name = exp_name\r\n",
        "\r\n",
        "    num_pixels = train_X.shape[1]*train_X.shape[2]\r\n",
        "\r\n",
        "    model = Nueral_net()\r\n",
        "\r\n",
        "    # intializing first hidden layer\r\n",
        "    model.add_layer( Layer ( num_inputs = train_X.shape[1]*train_X.shape[2] , num_nuerons = config.num_nuerons , activation = config.activation ,intilization = config.intialisation) )\r\n",
        "\r\n",
        "    # intializing all remaining hidden layer\r\n",
        "    for h in range(config.hidden_layers - 1):\r\n",
        "        model.add_layer( Layer ( num_inputs = config.num_nuerons , num_nuerons = config.num_nuerons, activation = config.activation, intilization = config.intialisation) )\r\n",
        "\r\n",
        "    # intializing output layer\r\n",
        "    model.add_layer( Layer ( num_inputs = config.num_nuerons, num_nuerons = 10 , activation = \"softmax\",intilization = config.intialisation) )\r\n",
        "\r\n",
        "    # input data\r\n",
        "    X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "    Y = train_Y[:50000]\r\n",
        "    \r\n",
        "    # validation data\r\n",
        "    validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "    validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "    model.fit( X, Y, validate_X, validate_Y, epochs = config.epochs, eta = config.eta, mini_batch_size = config.batch_size , optimizer = config.optimizer, gamma = config.gamma, epsilon = config.epsilon , beta1 = config.beta1, beta2 = config.beta2, alpha = config.alpha)\r\n",
        "\r\n",
        "    train_acc = model.accuracy( X, Y)\r\n",
        "    wandb.log( { \"accuracy\" : train_acc } )\r\n",
        "    \r\n",
        "    # test data\r\n",
        "    tst_X = (test_X.reshape( test_X.shape[0],-1).T)*(1.0/255)\r\n",
        "    tst_Y = test_Y\r\n",
        "\r\n",
        "    test_acc = model.accuracy( tst_X, tst_Y)\r\n",
        "    wandb.log( { \"test accuracy\" : test_acc } )"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "ZJu1YmzYr5mU",
        "outputId": "cc4f9db1-5084-40eb-f49c-ce927eb2fdcc"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ge43vt6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teta: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tintialisation: Xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_nuerons: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nesterov\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">celestial-sweep-61</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/sweeps/f3aad59d\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/sweeps/f3aad59d</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/8ge43vt6\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/8ge43vt6</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210311_204433-8ge43vt6</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4X0a_-976M"
      },
      "source": [
        "# Question 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "580o8rn1UU0i"
      },
      "source": [
        "# Mini-Batch Gradient Descent with cross entropy loss for local run\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "                # loss function with cross entropy\r\n",
        "                L = (np.sum( -np.log(mini_batch_y_bar.T[ np.arange(mini_batch_y_bar.shape[1]) , mini_batch_Y ] ) ) / m)\r\n",
        "                self.loss_list.append(L)\r\n",
        "    \r\n",
        "                \r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = ( - np.subtract(one_hot_vector.T , mini_batch_y_bar) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "\r\n",
        "        L = (np.sum( -np.log(y_bar.T[ np.arange(y_bar.shape[1]) , Y ] ) ) / Y.shape[0] )\r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqVRMLLo1I0p"
      },
      "source": [
        "# Local run with best tuned hyperparameter from above sweep\r\n",
        "\r\n",
        "model = Nueral_net()\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "hidden_layers = 3\r\n",
        "num_nuerons = 128\r\n",
        "alpha = 0.0005\r\n",
        "eta = 0.0001\r\n",
        "optimizer = \"nadam\"\r\n",
        "batch_size = 32\r\n",
        "intialisation = \"random\"\r\n",
        "activation = \"sigmoid\"\r\n",
        "beta1 = 0.9\r\n",
        "beta2 = 0.999\r\n",
        "gamma = 0.9\r\n",
        "epsilon = 1e-8\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = train_X.shape[1]* train_X.shape[2] , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "for h in range(hidden_layers):\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = num_nuerons, activation = activation,intilization = intialisation ) )\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\" ,intilization = intialisation) )\r\n",
        "\r\n",
        "\r\n",
        "X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "Y = train_Y[:50000]\r\n",
        " \r\n",
        "validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "model.fit(X, Y, validate_X, validate_Y, epochs , eta, batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMM_7k6XXTM4",
        "outputId": "4d36452d-b928-4d0b-c87d-f00cfe712ba5"
      },
      "source": [
        "# test accuracy\r\n",
        "X = test_X.reshape(test_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print( (np.sum( test_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1a4e443450634788afecf4567d3a9dea",
            "9ddc28b993cf4273805d75a09a731ced",
            "4e0fb9e6d2d447d68e515f5276f7addc",
            "8b90e31039a0489da8e8faac6adb66db",
            "31c1021c14dc4038a4603a29435959e4",
            "43f519b209604feaaa7a390dade3b4d1",
            "92db5e57b63046bdb9c5dbbf9ae223db",
            "3bb01fff9bb3401e9f1bc147b3008b8d"
          ]
        },
        "id": "aHzEGABQE5tj",
        "outputId": "7ac47a49-48b1-42c7-8fb1-daeae088646e"
      },
      "source": [
        "# plotting confusion matrix on test data\r\n",
        "fig, ax = plt.subplots(figsize=(15, 12))\r\n",
        "cf_matrix = confusion_matrix(y_bar, test_Y )\r\n",
        "\r\n",
        "xticklabels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "yticklabels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "sns.heatmap( cf_matrix, annot=True,fmt = \"d\", linewidth = 0.5,cmap='Blues', ax = ax, xticklabels = xticklabels , yticklabels = yticklabels )\r\n",
        "\r\n",
        "# Rotate the tick labels and set their alignment.\r\n",
        "plt.setp(ax.get_yticklabels(), rotation=0)\r\n",
        "\r\n",
        "ax.invert_yaxis()\r\n",
        "\r\n",
        "ax.set_xlabel('y_true',size=15)\r\n",
        "ax.set_ylabel('y_pred',size=15)\r\n",
        "ax.set_title(\"Confusion matrix\",size = 20)\r\n",
        "\r\n",
        "\r\n",
        "# Wandb config for question 7\r\n",
        "\r\n",
        "# Intialize a new run\r\n",
        "wandb.init(project=\"DL_assignment_1\", name=\"Question_7_confusion_matrix\")\r\n",
        "\r\n",
        "# Log the image\r\n",
        "wandb.log({\"Confusion_matrix\": [ wandb.Image( plt, caption = \"Confusion_matrix\"  )   ]})\r\n",
        "# wandb.log({\"Confusion_matrix\": plt})\r\n",
        "\r\n",
        "wandb.finish()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20s002\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">Question_7_confusion_matrix</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/rkeienxk\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/rkeienxk</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_052042-rkeienxk</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1263<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a4e443450634788afecf4567d3a9dea",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210314_052042-rkeienxk/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210314_052042-rkeienxk/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>3</td></tr><tr><td>_timestamp</td><td>1615699245</td></tr><tr><td>_step</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">Question_7_confusion_matrix</strong>: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/rkeienxk\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/rkeienxk</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2uzPcj--D03"
      },
      "source": [
        "# Question 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzMUNP3SFVjt"
      },
      "source": [
        "# Mini-Batch Gradient Descent with square error loss and wandb log\r\n",
        "\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "\r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # loss function with square error\r\n",
        "                L = np.sum(np.sum(np.square(np.subtract( mini_batch_y_bar, one_hot_vector.T )), axis=0 ))/m\r\n",
        "                self.loss_list.append(L)\r\n",
        "                \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = np.empty(shape=(10,m) )\r\n",
        "                for i in range(m):\r\n",
        "                    dL_aL[:,i] = np.multiply( mini_batch_y_bar[:,i] , np.dot( (np.identity(10) - mini_batch_y_bar[:,i]).T , 2*(mini_batch_y_bar[:,i] - one_hot_vector.T[:,i]) ) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "\r\n",
        "            wandb.log( { \"epoch\" : epoch } )\r\n",
        "            wandb.log( { \"training loss\" : self.cal_loss(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation loss\" : self.cal_loss(validate_X, validate_Y) } )\r\n",
        "            wandb.log( { \"training accuracy\" : self.accuracy(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation accuracy\" : self.accuracy(validate_X, validate_Y) } )\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "        \r\n",
        "        one_hot_vector = np.zeros(y_bar.T.shape)\r\n",
        "        rows = np.arange(y_bar.shape[1])\r\n",
        "        one_hot_vector[rows,Y] = 1\r\n",
        "    \r\n",
        "        # loss function with square error\r\n",
        "        L = np.sum(np.sum(np.square(np.subtract( y_bar, one_hot_vector.T )), axis=0 )) / Y.shape[0] \r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma805NbIjHG0",
        "outputId": "3879e90e-4378-4277-afa7-2dcc85032830"
      },
      "source": [
        "sweep_config = {\r\n",
        "    'method' : 'random',\r\n",
        "    'metric' : {\r\n",
        "        'name' : 'accuracy',\r\n",
        "        'goal' : 'maximize'\r\n",
        "    },\r\n",
        "    'parameters' : {\r\n",
        "        'epochs' : {\r\n",
        "            'values' : [5, 10]\r\n",
        "        }\r\n",
        "    }\r\n",
        "}\r\n",
        "sweep_id = wandb.sweep(sweep_config,  entity = \"cs20s002\", project = \"DL_assignment_1\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 53zvdnud\n",
            "Sweep URL: https://wandb.ai/cs20s002/DL_assignment_1/sweeps/53zvdnud\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z1tZM0GjKRr"
      },
      "source": [
        "def train():\r\n",
        "\r\n",
        "    epochs = 10\r\n",
        "    hidden_layers = 4\r\n",
        "    num_nuerons = 128\r\n",
        "    alpha = 0\r\n",
        "    eta = 0.0001\r\n",
        "    optimizer = \"rmsprop\"\r\n",
        "    batch_size = 64\r\n",
        "    intialisation = \"Xavier\"\r\n",
        "    activation = \"sigmoid\"\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    gamma = 0.9\r\n",
        "    epsilon = 1e-8\r\n",
        "\r\n",
        "    # wandb intialisation\r\n",
        "    wandb.init( project=\"DL_assignment_1\" )\r\n",
        "    \r\n",
        "    exp_name = \"square_error_loss\"+\"_hl_\" + str(hidden_layers) + \"_nn_\"+ str(num_nuerons) +  \"_ac_\" + str(activation) + \"_op_\" + str(optimizer) + \"_bs_\" + str(batch_size) \r\n",
        "    \r\n",
        "    wandb.run.name = exp_name\r\n",
        "\r\n",
        "    num_pixels = train_X.shape[1]*train_X.shape[2]\r\n",
        "\r\n",
        "    model = Nueral_net()\r\n",
        "\r\n",
        "    # intializing first hidden layer\r\n",
        "    model.add_layer( Layer ( num_inputs = train_X.shape[1]*train_X.shape[2] , num_nuerons = num_nuerons , activation = activation ,intilization = intialisation) )\r\n",
        "\r\n",
        "    # intializing all remaining hidden layer\r\n",
        "    for h in range(hidden_layers - 1):\r\n",
        "        model.add_layer( Layer ( num_inputs = num_nuerons , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "    # intializing output layer\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\", intilization = intialisation) )\r\n",
        "\r\n",
        "    # input data\r\n",
        "    X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "    Y = train_Y[:50000]\r\n",
        "    \r\n",
        "    # validation data\r\n",
        "    validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "    validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "    model.fit( X, Y, validate_X, validate_Y, epochs, eta, batch_size, optimizer , gamma , epsilon , beta1 , beta2 , alpha)\r\n",
        "\r\n",
        "    train_acc = model.accuracy( X, Y)\r\n",
        "    wandb.log( { \"accuracy\" : train_acc } )\r\n",
        "    \r\n",
        "    # test data\r\n",
        "    tst_X = (test_X.reshape( test_X.shape[0],-1).T)*(1.0/255)\r\n",
        "    tst_Y = test_Y\r\n",
        "\r\n",
        "    test_acc = model.accuracy( tst_X, tst_Y)\r\n",
        "    wandb.log( { \"test accuracy\" : test_acc } )"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e48916a1bed445deb235d13b87e3bd54",
            "29beec98759a4907ba1477a837aa6118",
            "513b52e6460b47998b8f0cd11fdeef0c",
            "e437c2afb93c40278c0df95503696bbc",
            "6e493623ffd84d8f9aefb6974f9fc86a",
            "8554a25024bb465bb578590765e09ae6",
            "76ee111af8ff493b8e4e236f9d8853f8",
            "80ed5e0bb31b44a6aaa0c3b7202954e5"
          ]
        },
        "id": "cQZuPcNSjUdb",
        "outputId": "9a058800-43cf-4e0e-a5cf-b69379d3d095"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eneqcum2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">hopeful-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/sweeps/53zvdnud\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/sweeps/53zvdnud</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/eneqcum2\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/eneqcum2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_081231-eneqcum2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1943<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "502 response executing GraphQL.\n",
            "\n",
            "<html><head>\n",
            "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
            "<title>502 Server Error</title>\n",
            "</head>\n",
            "<body text=#000000 bgcolor=#ffffff>\n",
            "<h1>Error: Server Error</h1>\n",
            "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
            "<h2></h2>\n",
            "</body></html>\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e48916a1bed445deb235d13b87e3bd54",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210314_081231-eneqcum2/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210314_081231-eneqcum2/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>9</td></tr><tr><td>_runtime</td><td>87</td></tr><tr><td>_timestamp</td><td>1615709638</td></tr><tr><td>_step</td><td>51</td></tr><tr><td>training loss</td><td>0.37704</td></tr><tr><td>validation loss</td><td>0.38529</td></tr><tr><td>training accuracy</td><td>72.088</td></tr><tr><td>validation accuracy</td><td>71.41</td></tr><tr><td>accuracy</td><td>72.088</td></tr><tr><td>test accuracy</td><td>71.31</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training loss</td><td>█▆▄▃▂▂▂▂▁▁</td></tr><tr><td>validation loss</td><td>█▆▄▃▂▂▂▂▁▁</td></tr><tr><td>training accuracy</td><td>▁▄▅▆▇▇████</td></tr><tr><td>validation accuracy</td><td>▁▄▅▆▇▇████</td></tr><tr><td>accuracy</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">hopeful-sweep-1</strong>: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/eneqcum2\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/eneqcum2</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUuetiURhw4M"
      },
      "source": [
        "# Mini-Batch Gradient Descent with cross entropy loss and wandb log \r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "                # loss function with cross entropy\r\n",
        "                L = (np.sum( -np.log(mini_batch_y_bar.T[ np.arange(mini_batch_y_bar.shape[1]) , mini_batch_Y ] ) ) / m)\r\n",
        "                self.loss_list.append(L)\r\n",
        "    \r\n",
        "                \r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = ( - np.subtract(one_hot_vector.T , mini_batch_y_bar) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "\r\n",
        "            wandb.log( { \"epoch\" : epoch } )\r\n",
        "            wandb.log( { \"training loss\" : self.cal_loss(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation loss\" : self.cal_loss(validate_X, validate_Y) } )\r\n",
        "            wandb.log( { \"training accuracy\" : self.accuracy(input_X, input_Y) } )\r\n",
        "            wandb.log( { \"validation accuracy\" : self.accuracy(validate_X, validate_Y) } )\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "\r\n",
        "        L = (np.sum( -np.log(y_bar.T[ np.arange(y_bar.shape[1]) , Y ] ) ) / Y.shape[0] )\r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n",
        "\r\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3RUcBnskhzF"
      },
      "source": [
        "def train():\r\n",
        "\r\n",
        "    epochs = 10\r\n",
        "    hidden_layers = 4\r\n",
        "    num_nuerons = 128\r\n",
        "    alpha = 0\r\n",
        "    eta = 0.0001\r\n",
        "    optimizer = \"rmsprop\"\r\n",
        "    batch_size = 64\r\n",
        "    intialisation = \"Xavier\"\r\n",
        "    activation = \"sigmoid\"\r\n",
        "    beta1 = 0.9\r\n",
        "    beta2 = 0.999\r\n",
        "    gamma = 0.9\r\n",
        "    epsilon = 1e-8\r\n",
        "\r\n",
        "    # wandb intialisation\r\n",
        "    wandb.init( project=\"DL_assignment_1\" )\r\n",
        "    \r\n",
        "    exp_name = \"cross_entropy_loss\"+\"_hl_\" + str(hidden_layers) + \"_nn_\"+ str(num_nuerons) +  \"_ac_\" + str(activation) + \"_op_\" + str(optimizer) + \"_bs_\" + str(batch_size) \r\n",
        "    \r\n",
        "    wandb.run.name = exp_name\r\n",
        "\r\n",
        "    num_pixels = train_X.shape[1]*train_X.shape[2]\r\n",
        "\r\n",
        "    model = Nueral_net()\r\n",
        "\r\n",
        "    # intializing first hidden layer\r\n",
        "    model.add_layer( Layer ( num_inputs = train_X.shape[1]*train_X.shape[2] , num_nuerons = num_nuerons , activation = activation ,intilization = intialisation) )\r\n",
        "\r\n",
        "    # intializing all remaining hidden layer\r\n",
        "    for h in range(hidden_layers - 1):\r\n",
        "        model.add_layer( Layer ( num_inputs = num_nuerons , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "    # intializing output layer\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\", intilization = intialisation) )\r\n",
        "\r\n",
        "    # input data\r\n",
        "    X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "    Y = train_Y[:50000]\r\n",
        "    \r\n",
        "    # validation data\r\n",
        "    validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "    validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "    model.fit( X, Y, validate_X, validate_Y, epochs, eta, batch_size, optimizer , gamma , epsilon , beta1 , beta2 , alpha)\r\n",
        "\r\n",
        "    train_acc = model.accuracy( X, Y)\r\n",
        "    wandb.log( { \"accuracy\" : train_acc } )\r\n",
        "    \r\n",
        "    # test data\r\n",
        "    tst_X = (test_X.reshape( test_X.shape[0],-1).T)*(1.0/255)\r\n",
        "    tst_Y = test_Y\r\n",
        "\r\n",
        "    test_acc = model.accuracy( tst_X, tst_Y)\r\n",
        "    wandb.log( { \"test accuracy\" : test_acc } )"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821,
          "referenced_widgets": [
            "c9dd1ef479134753998681680fc77f70",
            "9c2cb07c5996401591d9ad5bebd930c9",
            "dc9760c0705e43b4921708303f758fcc",
            "42eea7a54fc6405eace002a8d2e25478",
            "70592613a4724b1e8faa47758c2f487d",
            "e65197daece04acdae3fd4369fa409e5",
            "a6717885b6104cbe967e70da9ebeb371",
            "ab61529c64d9444585bd0bce09096aba"
          ]
        },
        "id": "aq8PNIYYkjks",
        "outputId": "faef7d5f-853c-4193-d394-17ec7646c96b"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kwuy3rp2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">pious-sweep-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/sweeps/53zvdnud\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/sweeps/53zvdnud</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/kwuy3rp2\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/kwuy3rp2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_081452-kwuy3rp2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1980<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9dd1ef479134753998681680fc77f70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210314_081452-kwuy3rp2/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210314_081452-kwuy3rp2/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>9</td></tr><tr><td>_runtime</td><td>82</td></tr><tr><td>_timestamp</td><td>1615709774</td></tr><tr><td>_step</td><td>51</td></tr><tr><td>training loss</td><td>0.51027</td></tr><tr><td>validation loss</td><td>0.52673</td></tr><tr><td>training accuracy</td><td>81.316</td></tr><tr><td>validation accuracy</td><td>80.42</td></tr><tr><td>accuracy</td><td>81.316</td></tr><tr><td>test accuracy</td><td>80.37</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training loss</td><td>█▆▅▃▂▂▂▁▁▁</td></tr><tr><td>validation loss</td><td>█▆▅▃▂▂▂▁▁▁</td></tr><tr><td>training accuracy</td><td>▁▃▄▆▇▇████</td></tr><tr><td>validation accuracy</td><td>▁▃▄▆▇▇▇███</td></tr><tr><td>accuracy</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">pious-sweep-2</strong>: <a href=\"https://wandb.ai/cs20s002/DL_assignment_1/runs/kwuy3rp2\" target=\"_blank\">https://wandb.ai/cs20s002/DL_assignment_1/runs/kwuy3rp2</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jebuWj6sXEES"
      },
      "source": [
        "# Question 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_M4qlRjYdL4"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "\r\n",
        "# Loading dataset\r\n",
        "df = mnist.load_data()\r\n",
        "((train_X, train_Y), (test_X, test_Y)) = df"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5XMnah-XHHy"
      },
      "source": [
        "# Mini-Batch Gradient Descent with cross entropy loss for mnist dataset\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "                # loss function with cross entropy\r\n",
        "                L = (np.sum( -np.log(mini_batch_y_bar.T[ np.arange(mini_batch_y_bar.shape[1]) , mini_batch_Y ] ) ) / m)\r\n",
        "                self.loss_list.append(L)\r\n",
        "    \r\n",
        "                \r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = ( - np.subtract(one_hot_vector.T , mini_batch_y_bar) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "\r\n",
        "        L = (np.sum( -np.log(y_bar.T[ np.arange(y_bar.shape[1]) , Y ] ) ) / Y.shape[0] )\r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n",
        "\r\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1nDRp7YXtkx",
        "outputId": "0caecb84-5f36-495c-cca7-f171c1e14c1c"
      },
      "source": [
        "# Local run with best tuned hyperparameter from above sweep for mnist dataset\r\n",
        "\r\n",
        "model = Nueral_net()\r\n",
        "\r\n",
        "# 1st configuration\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "hidden_layers = 3\r\n",
        "num_nuerons = 128\r\n",
        "alpha = 0.0005\r\n",
        "eta = 0.0001\r\n",
        "optimizer = \"nadam\"\r\n",
        "batch_size = 32\r\n",
        "intialisation = \"random\"\r\n",
        "activation = \"sigmoid\"\r\n",
        "beta1 = 0.9\r\n",
        "beta2 = 0.999\r\n",
        "gamma = 0.9\r\n",
        "epsilon = 1e-8\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = train_X.shape[1]* train_X.shape[2] , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "for h in range(hidden_layers):\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = num_nuerons, activation = activation,intilization = intialisation ) )\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\" ,intilization = intialisation) )\r\n",
        "\r\n",
        "\r\n",
        "X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "Y = train_Y[:50000]\r\n",
        " \r\n",
        "validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "model.fit(X, Y, validate_X, validate_Y, epochs , eta, batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha)\r\n",
        "\r\n",
        "# train accuracy\r\n",
        "X = train_X.reshape(train_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"1st configuration Train accuracy : \",(np.sum( train_Y == y_bar )/y_bar.shape[0])*100  )\r\n",
        "\r\n",
        "# test accuracy\r\n",
        "X = test_X.reshape(test_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"1st configuration Test accuracy : \",(np.sum( test_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1st configuration Train accuracy :  98.46833333333333\n",
            "1st configuration Test accuracy :  96.88\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOewFTqd7g3Z",
        "outputId": "b67b7d17-a356-459e-a672-1da267806277"
      },
      "source": [
        "# Local run with best tuned hyperparameter from above sweep for mnist dataset\r\n",
        "\r\n",
        "model = Nueral_net()\r\n",
        "\r\n",
        "# 2nd configuration\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "hidden_layers = 4\r\n",
        "num_nuerons = 128\r\n",
        "alpha = 0\r\n",
        "eta = 0.0001\r\n",
        "optimizer = \"rmsprop\"\r\n",
        "batch_size = 64\r\n",
        "intialisation = \"Xavier\"\r\n",
        "activation = \"relu\"\r\n",
        "beta1 = 0.9\r\n",
        "beta2 = 0.999\r\n",
        "gamma = 0.9\r\n",
        "epsilon = 1e-8\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = train_X.shape[1]* train_X.shape[2] , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "for h in range(hidden_layers):\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = num_nuerons, activation = activation,intilization = intialisation ) )\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\" ,intilization = intialisation) )\r\n",
        "\r\n",
        "\r\n",
        "X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "Y = train_Y[:50000]\r\n",
        " \r\n",
        "validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "model.fit(X, Y, validate_X, validate_Y, epochs , eta, batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha)\r\n",
        "\r\n",
        "# train accuracy\r\n",
        "X = train_X.reshape(train_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"2nd configuration Train accuracy : \",(np.sum( train_Y == y_bar )/y_bar.shape[0])*100  )\r\n",
        "\r\n",
        "# test accuracy\r\n",
        "X = test_X.reshape(test_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"2nd configuration Test accuracy : \",(np.sum( test_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2nd configuration Train accuracy :  98.34\n",
            "2nd configuration Test accuracy :  96.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE7P3lR9YurR",
        "outputId": "90474e34-88cd-4273-ed97-0515c81111a5"
      },
      "source": [
        "# Local run with best tuned hyperparameter from above sweep for mnist dataset\r\n",
        "\r\n",
        "model = Nueral_net()\r\n",
        "\r\n",
        "# 3rd configuration\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "hidden_layers = 3\r\n",
        "num_nuerons = 128\r\n",
        "alpha = 0\r\n",
        "eta = 0.0001\r\n",
        "optimizer = \"rmsprop\"\r\n",
        "batch_size = 64\r\n",
        "intialisation = \"Xavier\"\r\n",
        "activation = \"tanh\"\r\n",
        "beta1 = 0.9\r\n",
        "beta2 = 0.999\r\n",
        "gamma = 0.9\r\n",
        "epsilon = 1e-8\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = train_X.shape[1]* train_X.shape[2] , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "for h in range(hidden_layers):\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = num_nuerons, activation = activation,intilization = intialisation ) )\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\" ,intilization = intialisation) )\r\n",
        "\r\n",
        "\r\n",
        "X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "Y = train_Y[:50000]\r\n",
        " \r\n",
        "validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "model.fit(X, Y, validate_X, validate_Y, epochs , eta, batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha)\r\n",
        "\r\n",
        "# train accuracy\r\n",
        "X = train_X.reshape(train_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"3rd configuration Train accuracy : \",(np.sum( train_Y == y_bar )/y_bar.shape[0])*100  )\r\n",
        "\r\n",
        "# test accuracy\r\n",
        "X = test_X.reshape(test_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print(\"3rd configuration Test accuracy : \",(np.sum( test_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3rd configuration Train accuracy :  97.55\n",
            "3rd configuration Test accuracy :  96.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg4WX1h9-JSz"
      },
      "source": [
        "# Extra local test runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raee_oTY4jOH"
      },
      "source": [
        "# Mini-Batch Gradient Descent with cross entropy loss for mnist dataset\r\n",
        "random.seed(0)\r\n",
        "\r\n",
        "class Layer:\r\n",
        "\r\n",
        "    def __init__(self, num_inputs, num_nuerons, activation, intilization):\r\n",
        "        \r\n",
        "        if intilization == \"random\":\r\n",
        "            \r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-0.5,0.5,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias_nag = (np.random.uniform(-0.5,0.5,(num_nuerons)))\r\n",
        "\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "        if intilization == \"Xavier\":\r\n",
        "\r\n",
        "            self.activation = activation\r\n",
        "\r\n",
        "            limit = np.sqrt(6/ (num_nuerons + num_inputs) )\r\n",
        "\r\n",
        "            self.weights = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )\r\n",
        "            self.bias = (np.random.uniform(-limit,limit,(num_nuerons)))\r\n",
        "\r\n",
        "            # for gradients\r\n",
        "            self.weights_grad = None\r\n",
        "            self.bias_grad = None\r\n",
        "            \r\n",
        "            # for storing forward pass\r\n",
        "            self.aL = None\r\n",
        "            self.hL = None\r\n",
        "\r\n",
        "            # for NAG, Moment ,Adam\r\n",
        "            self.prev_update_weights = np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias = np.zeros(num_nuerons)\r\n",
        "            \r\n",
        "            # for NAG\r\n",
        "            self.weights_nag = (np.random.uniform(-limit,limit,(num_nuerons,num_inputs)) )/1\r\n",
        "            self.bias_nag = (np.random.uniform(-limit,limit,(num_nuerons)))/1\r\n",
        "\r\n",
        "            # for Adagrad,RMSprop,Adam\r\n",
        "            self.prev_update_weights_grad_square =  np.zeros([num_nuerons,num_inputs])\r\n",
        "            self.prev_update_bias_grad_square =  np.zeros(num_nuerons)\r\n",
        "\r\n",
        "class Nueral_net:\r\n",
        "  \r\n",
        "    def __init__(self,):\r\n",
        "        self.layers = []\r\n",
        "        self.outputs = None\r\n",
        "        self.loss_list = []\r\n",
        "\r\n",
        "    def add_layer(self,layer):\r\n",
        "        self.layers.append(layer)\r\n",
        "\r\n",
        "    def fit(self, X, Y, validate_X, validate_Y, epochs, eta, mini_batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha):\r\n",
        "        \r\n",
        "        # iterating over epochs\r\n",
        "        for epoch in range(epochs):\r\n",
        "            \r\n",
        "            # input_X shape = ( number_of_features, number_of_examples)\r\n",
        "            input_X = X \r\n",
        "            # input_Y shape = ( number_of_examples, )\r\n",
        "            input_Y = Y\r\n",
        "\r\n",
        "            mini_batch_size = mini_batch_size\r\n",
        "            no_of_batches = X.shape[1] / mini_batch_size\r\n",
        "\r\n",
        "            # iterating over batches\r\n",
        "            for batch in range(int(no_of_batches)):\r\n",
        "                \r\n",
        "                mini_batch_X = X [ : , mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                mini_batch_Y = Y [  mini_batch_size*(batch) : mini_batch_size*(batch+1) ]\r\n",
        "                \r\n",
        "                mini_batch_input = mini_batch_X\r\n",
        "\r\n",
        "\r\n",
        "                # forward propogation                \r\n",
        "                for layer in self.layers: \r\n",
        "          \r\n",
        "                    layer.aL = self.apply_linear_sum(layer.weights, layer.bias, mini_batch_X )\r\n",
        "                    \r\n",
        "                    layer.hL = self.apply_activation_function( layer.aL, layer.activation ) \r\n",
        "                    \r\n",
        "                    mini_batch_X = layer.hL\r\n",
        "\r\n",
        "                mini_batch_y_bar = mini_batch_X\r\n",
        "                \r\n",
        "                # m = number of examples in batch \r\n",
        "                m = mini_batch_y_bar.shape[1]\r\n",
        "\r\n",
        "                # loss function with cross entropy\r\n",
        "                L = (np.sum( -np.log(mini_batch_y_bar.T[ np.arange(mini_batch_y_bar.shape[1]) , mini_batch_Y ] ) ) / m)\r\n",
        "                self.loss_list.append(L)\r\n",
        "    \r\n",
        "                \r\n",
        "                # back propogation\r\n",
        "\r\n",
        "                # creating one hot vector for every examples\r\n",
        "                # shape of one hot vector = (number of examples, number of output units)\r\n",
        "                one_hot_vector = np.zeros(mini_batch_y_bar.T.shape)\r\n",
        "                rows = np.arange(mini_batch_y_bar.shape[1])\r\n",
        "                one_hot_vector[rows,mini_batch_Y] = 1\r\n",
        "            \r\n",
        "                # gradients for the output layer \r\n",
        "                dL_aL = ( - np.subtract(one_hot_vector.T , mini_batch_y_bar) )\r\n",
        "        \r\n",
        "                # gradients for the hidden layer\r\n",
        "                for layer in list(range(1,len(self.layers)))[::-1] :\r\n",
        "\r\n",
        "                    # gradients with respect to parameters\r\n",
        "                    self.layers[layer].weights_grad = np.dot( dL_aL , self.layers[layer-1].hL.T ) / m\r\n",
        "                    self.layers[layer].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "                    \r\n",
        "                    # gradients with respect to layer below\r\n",
        "                    dL_hL_minus_1 = np.dot(self.layers[layer].weights.T, dL_aL)\r\n",
        "                    dL_aL = np.multiply(dL_hL_minus_1 , self.apply_activation_function_derivative( self.layers[layer-1].aL, self.layers[layer-1].hL, self.layers[layer-1].activation, m ))\r\n",
        "                    \r\n",
        "                # gradients of 1st layer\r\n",
        "                self.layers[0].weights_grad = np.dot( dL_aL , mini_batch_input.T )/m\r\n",
        "                self.layers[0].bias_grad = np.sum(dL_aL, axis=1)/m\r\n",
        "\r\n",
        "                # update weights\r\n",
        "                self.update_parameters(eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch)\r\n",
        "                # applying regularization\r\n",
        "                self.apply_reg(eta, alpha)\r\n",
        "            \r\n",
        "    def apply_reg(self, eta, alpha):\r\n",
        "\r\n",
        "        for layer in self.layers:\r\n",
        "            \r\n",
        "            layer.weights = np.subtract( layer.weights, (eta*alpha)*layer.weights )\r\n",
        "            layer.bias = np.subtract( layer.bias, (eta*alpha)*layer.bias )\r\n",
        "        \r\n",
        "    def cal_loss(self, X , Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = X\r\n",
        "\r\n",
        "        L = (np.sum( -np.log(y_bar.T[ np.arange(y_bar.shape[1]) , Y ] ) ) / Y.shape[0] )\r\n",
        "        \r\n",
        "        return L\r\n",
        "    \r\n",
        "    def update_parameters(self, eta, optimizer, gamma, epoch, epochs, epsilon, beta1, beta2, batch):\r\n",
        "        \r\n",
        "        if optimizer == \"momentum\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                moment_update_weights = np.add( gamma*layer.prev_update_weights , eta*layer.weights_grad) \r\n",
        "                moment_update_bias = np.add( gamma*layer.prev_update_bias , eta*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights, moment_update_weights )\r\n",
        "                layer.bias = np.subtract( layer.bias, moment_update_bias )\r\n",
        "\r\n",
        "                layer.prev_update_weights = moment_update_weights\r\n",
        "                layer.prev_update_bias = moment_update_bias\r\n",
        "        \r\n",
        "        if optimizer == \"nesterov\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add(gamma*layer.prev_update_weights , eta*layer.weights_grad)\r\n",
        "                layer.prev_update_bias = np.add(gamma*layer.prev_update_bias , eta*layer.bias_grad)\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract(layer.weights_nag , layer.prev_update_weights )\r\n",
        "                layer.bias_nag = np.subtract(layer.bias_nag , layer.prev_update_bias )\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , gamma*layer.prev_update_weights )\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , gamma*layer.prev_update_bias )\r\n",
        "\r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "                \r\n",
        "        if optimizer == \"AdaGrad\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add(layer.self.prev_update_weights_grad_square, (layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add(layer.self.prev_update_bias_grad_square, (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer== \"rmsprop\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        " \r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(layer.prev_update_weights_grad_square + epsilon) ) * layer.weights_grad)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(layer.prev_update_bias_grad_square + epsilon) ) * layer.bias_grad)\r\n",
        "\r\n",
        "        if optimizer == \"adam\":\r\n",
        "\r\n",
        "            for layer in self.layers:\r\n",
        "                \r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "                \r\n",
        "        if optimizer == \"nadam\":\r\n",
        "            \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.prev_update_weights = np.add( beta1 * layer.prev_update_weights , (1-beta1)*layer.weights_grad) \r\n",
        "                layer.prev_update_bias = np.add( beta1 * layer.prev_update_bias , (1-beta1)*layer.bias_grad )\r\n",
        "\r\n",
        "                layer.prev_update_weights_grad_square = np.add( beta2 * layer.prev_update_weights_grad_square, (1 - beta2) * ( layer.weights_grad)**2 )\r\n",
        "                layer.prev_update_bias_grad_square = np.add( beta2 * layer.prev_update_bias_grad_square, (1 - beta2) * (layer.bias_grad)**2 ) \r\n",
        "\r\n",
        "                # bias correction\r\n",
        "                prev_update_weights_normalize = layer.prev_update_weights/( 1 - math.pow( beta1, (epoch+1 )*(batch+1) ) )\r\n",
        "                prev_update_bias_normalize = layer.prev_update_bias/( 1 - math.pow( beta1, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                prev_update_weights_grad_square_normalize = layer.prev_update_weights_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "                prev_update_bias_grad_square_normalize = layer.prev_update_bias_grad_square/( 1 - math.pow( beta2, (epoch+1)*(batch+1) ) )\r\n",
        "\r\n",
        "                layer.weights_nag = np.subtract( layer.weights , (eta / np.sqrt(prev_update_weights_grad_square_normalize + epsilon) ) * prev_update_weights_normalize)\r\n",
        "                layer.bias_nag = np.subtract( layer.bias , (eta / np.sqrt(prev_update_bias_grad_square_normalize + epsilon) ) * prev_update_bias_normalize)\r\n",
        "\r\n",
        "                \r\n",
        "                # for next round\r\n",
        "                \r\n",
        "                # setting look_aheads\r\n",
        "                layer.weights = np.subtract(layer.weights_nag , beta1*prev_update_weights_normalize)\r\n",
        "                layer.bias = np.subtract(layer.bias_nag , beta1*prev_update_bias_normalize)\r\n",
        "                \r\n",
        "            if epoch == (epochs-1):\r\n",
        "\r\n",
        "                layer.weights = layer.weights_nag\r\n",
        "                layer.bias = layer.bias_nag\r\n",
        "\r\n",
        "        if optimizer == \"sgd\":\r\n",
        "                \r\n",
        "            for layer in self.layers:\r\n",
        "\r\n",
        "                layer.weights = np.subtract(layer.weights , eta*layer.weights_grad)\r\n",
        "                layer.bias = np.subtract(layer.bias , eta*layer.bias_grad )\r\n",
        "                \r\n",
        "    def accuracy(self, X, Y):\r\n",
        "\r\n",
        "        for layer in self.layers:        \r\n",
        "            linear_sum = self.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "            X = self.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "        y_bar = np.argmax( X ,axis = 0)\r\n",
        "\r\n",
        "        return ( np.sum( Y == y_bar )/y_bar.shape[0])*100\r\n",
        "\r\n",
        "    def apply_linear_sum( self, weights, bias, X ):\r\n",
        "        linear_sum = np.dot( weights, X ) + bias.reshape( bias.shape[0], 1 )\r\n",
        "        return linear_sum\r\n",
        "\r\n",
        "    def apply_activation_function( self, linear_sum, activation_fun ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            linear_sum = np.where( linear_sum > 0 , linear_sum, linear_sum*0.01 )\r\n",
        "            return linear_sum\r\n",
        "        \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return (1 / (1 + np.exp( - linear_sum) ) )\r\n",
        "        \r\n",
        "        if activation_fun == \"softmax\":\r\n",
        "            return (np.exp(linear_sum) / np.sum(np.exp(linear_sum), axis=0))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return ( (np.exp( linear_sum) - np.exp( - linear_sum) ) / (np.exp( linear_sum) + np.exp( - linear_sum) ) )\r\n",
        "\r\n",
        "    def apply_activation_function_derivative( self, aL, hL, activation_fun, m ):\r\n",
        "        \r\n",
        "        if activation_fun == \"relu\":\r\n",
        "            aL = np.where(aL >= 0, 1, 0.01) \r\n",
        "            return aL\r\n",
        "            \r\n",
        "        if activation_fun == \"sigmoid\":\r\n",
        "            return np.multiply(hL, (1-hL))\r\n",
        "        \r\n",
        "        if activation_fun == \"tanh\":\r\n",
        "            return (1 - (hL)**2)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6O4DnjVk9qb"
      },
      "source": [
        "# TEST RUN\r\n",
        "model = Nueral_net()\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "hidden_layers = 3\r\n",
        "num_nuerons = 128\r\n",
        "alpha = 0.0005\r\n",
        "eta = 0.0001\r\n",
        "optimizer = \"adam\"\r\n",
        "batch_size = 32\r\n",
        "intialisation = \"random\"\r\n",
        "activation = \"sigmoid\"\r\n",
        "beta1 = 0.9\r\n",
        "beta2 = 0.999\r\n",
        "gamma = 0.9\r\n",
        "epsilon = 1e-8\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = train_X.shape[1]* train_X.shape[2] , num_nuerons = num_nuerons, activation = activation, intilization = intialisation) )\r\n",
        "\r\n",
        "for h in range(hidden_layers):\r\n",
        "    model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = num_nuerons, activation = activation,intilization = intialisation ) )\r\n",
        "\r\n",
        "model.add_layer( Layer ( num_inputs = num_nuerons, num_nuerons = 10 , activation = \"softmax\" ,intilization = intialisation) )\r\n",
        "\r\n",
        "\r\n",
        "X = ((train_X.reshape( train_X.shape[0],-1).T)[:,:50000])*(1.0/255)\r\n",
        "Y = train_Y[:50000]\r\n",
        " \r\n",
        "validate_X = ((train_X.reshape( train_X.shape[0],-1).T)[:,50000:])*(1.0/255)\r\n",
        "validate_Y = train_Y[50000:]\r\n",
        "    \r\n",
        "model.fit(X, Y, validate_X, validate_Y, epochs , eta, batch_size, optimizer, gamma, epsilon, beta1, beta2, alpha)\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RmtGdcUlNFQ",
        "outputId": "71f15efd-bb9b-4d2e-c73a-86845a241a8b"
      },
      "source": [
        "\r\n",
        "# train accuracy\r\n",
        "X = train_X.reshape(train_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print( (np.sum( train_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "85.96166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukILf90fmngP",
        "outputId": "ad6444dd-48d9-4018-a33d-2ddc61e2fa72"
      },
      "source": [
        "# test accuracy\r\n",
        "X = test_X.reshape(test_X.shape[0],-1).T/255.\r\n",
        "\r\n",
        "for layer in model.layers:        \r\n",
        "  linear_sum = model.apply_linear_sum(layer.weights, layer.bias, X )\r\n",
        "  X = model.apply_activation_function( linear_sum , layer.activation) \r\n",
        "\r\n",
        "y_bar = np.argmax( X ,axis = 0)\r\n",
        "print( (np.sum( test_Y == y_bar )/y_bar.shape[0])*100  )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27SAw8vGmrUQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}